# Chapter_1. Linux's basic principle
  * Everything is file
    * This is the design philosophy of UNIX and Linux. Like all operating systems, executable files are stored on the hard drive. During boot, the BIOS and bootloader load these kernel executable files into memory, then transfer control to the kernel.
  * Embedded Systems
    * Some computers have limited storage space, such as embedded systems. These systems have limited hard drive space and can only use a stripped-down version of Linux, such as the commonly used embedded Linux Yocto. 
    * Mobile phones and portable devices also use embedded systems. For example, Android is essentially an embedded system, although it is much more powerful than typical low-end embedded systems. 
    * An embedded system is typically designed for specific tasks and has limited resources, requiring real-time responses. It is used to integrate into larger systems, such as printers, washing machines, smart home appliances, or industrial robots. These systems usually do not have traditional hard drives to store operating systems but use flash memory (like USB drives or SD cards, some are removable, some are embedded in the circuit board) to store the operating system. 
  * General-Purpose Computers
    * General-purpose computers are designed to perform multiple tasks with abundant resources. (To learn more about embedded systems, you can look up QEMU or Yocto.)

# Chapter_2. Important Files (Directories) in the Root Directory of Linux 
  * ## 2A. **/usr**
    * Short for "unix system resource". Most system-provided "executable files (and those you later install)", library files, shared data, and common commands (considered executable files) are located in this directory, including various common commands. 
    * It's important to note that everything in /usr is at the "system level", meaning it is oriented to all users. 
    * Here are some important files in /usr 
      * **/usr/bin**: Most of the system-provided executables, as well as those installed via apt or other software package managers, are located here. For example, when you use "apt install vim", it downloads the vim executable from the internet and stores it in "/usr/bin", which is already in the $PATH. This allows you to use the executable directly. If you remove "/usr/bin" from $PATH, you won't be able to perform most operations, such as cd, mv, cp, ls, etc. However, modifying $PATH with export is not permanent; it is only effective for the current session and reverts once you exit the terminal session, which is a safeguard. If you accidentally remove "/usr/bin" from $PATH, you can still invoke commands using the full path, such as /usr/bin/ls for "ls". Note that most installed executables are just binaries, and the source code can be found on GitHub.
      * **/usr/sbin**: Stores system administration commands and applications, usually requiring superuser (root) permissions to execute. Example commands include fsck, ifconfig, and iptables. If you use a command that requires sudo, it is likely located in /usr/sbin. 
      * **/usr/lib**: Stores library files that provide necessary functions and capabilities for executables in /usr/bin and other directories to run properly. Generally, executables (programs) in /usr/bin and other directories utilize common libraries, such as ls and cp, which might share certain functions. Static libraries (.a files) are included in program A when needed, whereas dynamic libraries (.so files, shared objects) interact dynamically with program B when necessary. Windows has equivalent files, known as dll files, which correspond to .so files. 
      * **/usr/local**: Typically used to store manually compiled and installed applications and libraries after system installation. These files are usually not managed by the system package manager. For example, if something is installed using npm instead of apt, it will be stored here. Subdirectories mirror the structure of /usr, such as /usr/local/bin, /usr/local/sbin, /usr/local/lib. 
      * **/usr/share**, To fully understand this directory, it's essential to know how Linux operates in multi-user mode. Long ago, before personal computers were common, a company or team typically had only one computer, with each team member connecting to it using their terminals. To differentiate users and their permissions, Linux gives every file a **operation permissions** for different users. The root user has absolute full permissions, allowing them to do anything, and all users are created by the root user. To enter a directory, you must have **execution permission** for that directory (the x bit in the inode). Without execution permission, you theoretically cannot enter its subdirectories, even if you have permission for those subdirectories. For instance, to enter dir/subdir, you need execution permission for dir; otherwise, you cannot enter subdir from dir unless you are already in dir and use a relative path, such as cd subdir. Here is a [reference discussion.](https://unix.stackexchange.com/questions/13858/do-the-parent-directorys-permissions-matter-when-accessing-a-subdirectory). Bob's home directory is in /home/bob, accessible only by Bob and root. When a new user is created (Bob's basic account settings are stored in /etc/passwd), the default home directory permissions are set to 700, meaning the owner (Bob) has read, write, and execute permissions (rwx). Users in the same group and other users have no permissions. These settings can be modified by root, allowing access to /home/bob for Alice if desired. The purpose of /usr/share is to provide a default folder for all users to share **system-level** **static files**, such as code, text, and image files. To reiterate, everything in /usr is **system-level**. If Bob installs something using apt, it stores executables in /usr/bin and static files in /usr/share. Apt is a "system-level" installation manager, serving the system rather than specific users. This is why apt requires root permissions and must be used with sudo. If Bob wants to install personal applications, he should create a bin folder in /home/bob and download executables there for personal use, which would not be "system-level" but rather for Bob only. Only contents in /home/bob serve Bob specifically, though root also has full permissions. 
      * **/usr/include** 
        * This directory contains C language header files (.h files). Although Linux itself is written in C, this directory has no relation to Linux operation itself. It is purely a directory for developers, containing numerous .h files, such as stdio.h and stdlib.h, essential for C and C++ development. When you use gcc to compile a .c file, #include <stdio.h> automatically searches /usr/include for .h files, where the < > are shortcuts for /usr/include. A question arises: header files declare functions but do not define them. Where are the defining files? They are all stored in /usr/lib as executable files (all compiled, source code can be found on GitHub), especially in the dynamic library "/usr/lib/x86_64-linux-gnu/libc.so". It includes executable(compiled) files for well defined-functions declared in stdio.h, stdlib.h, string.h, math.h, time.h, and error.h.
        * If you write a simple hello program in Linux using #include <XXXXX.h>, it will automatically search /usr/include for .h files and then /usr/lib for corresponding executable files (e.g., for stdio.h, the well-defined executable functions are in /usr/lib/x86_64-linux-gnu/libc.so). If you include OOOOO.h, it will compile the corresponding function-defined file (e.g., your XXX.c file) into the final output file, out.o. If you write a func.h file and a corresponding func.c file, compile it to get out.o, out.o will contain the compiled content of func.c.
        * If you write #include <stdio.h> printf("hello, world\n");, compile it to create an executable out.o, and put out.o on a machine without libc.so, out.o will not run because printf function implementation which is in libc.so cannot be found. 
        * Additionally, libc.so is not only for developers; many common commands in Linux, such as "ls", "mv", "cp", and "cd", rely on the standard C library (compiled as libc.so). If you rename, move or even remove libc.so, many commands will fail due to missing dependencies, as explained in this [real-life example](https://stackoverflow.com/questions/39885343/cannot-run-any-commands-because-i-moved-the-libc-so-file).
        * The linkage between executables and required dynamic libraries (.so files) is managed by ld.so(usually located in /usr/lib or related directories). You can use ldd /bin/ls to check dependencies and see the path to ld.so.ld.sosearches /usr/lib by default. Given the vast number of libraries in /usr/lib, wouldn't ld.sobe slow? ldconfig builds ld.so.cache, a binary file using special algorithms and data structures (e.g., hash or index tree), making ld.so much faster. The fastest method is specifying library paths(e.g. /usr/lib/libc.so) directly in the executable file, using rpath (specified during compilation, fixed) or runpath (specified at runtime, potentially risky while the input data has some malware code). However, fixed paths mean the library file's location cannot be moved, otherwise, it might become unusable. But here's another problem Copilot says about the disavantage about using fixed path. It said that executable files need to recompile when the libs get updated(if you're using rpath). But why? If you're using ld.so.cache without rpath, if there are some significant changes occur in the library, the executable might also need recompiling to meet the new requirements. So both way need to recompile then there's no advantage of ld.so.cache?
  * ## 2B. **/etc**, 
    * The name came from the early UNIX systems' "et cetera". Another interpretation is "editable text configuration". It mostly contains configuration files (config files) or scripts related to "system-level". They are mostly plain text files, including configuration files for system-level applications you install. For instance, if you install nginx, its configuration file should be located at /etc/nginx/nginx.conf. If you install vim using apt, its config will be in /etc/vim/vimrc (although vim allows specific users to set up their vimrc in their home directory, such as /home/user_name/vimrc, vim will prioritize this path). Some basics subdirectories include: 
      * User-related configuration files:
        * **/etc/passwd**: A plain text file storing user account information, including various process accounts (e.g., systemd-network, syslog, etc.). Its standard format is roughly as follows -> sara:x:1000:1000:Sara Z:/home/sara:/bin/bash. It uses colons (":") as delimiters, representing sara(username):x(account password, stored in /etc/shadow):1000(user id):1000(group id):Sara Z(GECOS):/home/sara(user's home directory):/bin/bash(user's default shell). /etc/passwd is the system's only dependency for "username", "user ID", "group ID", "home directory", and "default shell". Modifications will take effect immediately. For example, changing sara:x:1000:1000:Sara Z:/home/sara:/bin/bash to sara:x:1000:1000:Sara Z:/home/sara:/bin/zsh will make Sara use zsh upon the next login. It functions like a database storing user account information and corresponding properties, including usernames and passwords, all directly relied upon by the system, every modifications will work directly. 
        * **/etc/group**: Stores group information. 
        * **/etc/shadow**: Stores user passwords and related security information. 
        * Note that modifying user configuration files does not equate to changing user access permissions for specific files, as file permissions are stored in the "inode (index node)", a part of the file system. They are related but distinct. 
      * Service and daemon configurations:
        * **/etc/int.d/**: Stores service scripts executed during system startup and shutdown.
		* **/etc/systemd/**: For systems using systemd, their service and daemon configurations are stored here. 
	  * Network configurations
		* **/etc/hostname**: The system hostname configuration file. Basically this file contains your hostname in txt, if you change the hostname in it, it will work to whole system! 
		* **/etc/hosts**: The local hostname resolution file, acting as a "local DNS", mapping domain names to IP addresses (e.g., 192.168.1.10 example.com). Useful when DNS servers are unavailable or for quick local testing. 
		* **/etc/networks**: Stores network settings and interface configuration files. 
		* **/etc/iptables**: Stores rule files for filtering network packets (effective after reloading into memory or system restart), considered as the most native Linux firewall system.
      * Device and resource configurations 
		* **/etc/fstab**: The file system mount point configuration file. 
		* **/etc/mtab**: Information on currently mounted file systems. 
      * Program and application configurations 
		* **/etc/apt/**: Configuration files for the APT package management tool (applicable to Debian, Ubuntu, etc.). 
		* **/etc/yum.conf**: Configuration files for the YUM package management tool (applicable to Red Hat, CentOS, etc.). 

  * ## 2C. **/home**
    * Stores the personal home directories of users. Each user has their own directory here, such as /home/user_name. This is where users store their personal files and settings. 

  * ## 2D. **/dev**
    * means "devices", not "development". It contains files related to "hardware devices" and "pseudo devices". 
    * In the Linux file system, every hardware device is represented as a file. For example, a hard drive is represented as /dev/sda, and terminal devices might be represented as /tty1. System programs can interact directly with these device files. For example, to read from a hard drive, you can use the following command: dd if=/dev/sda of=/path/to/output/file bs=512 count=1, which reads the first 512 bytes from /dev/sda and writes them to the specified file. Another example is writing data to /dev/null: echo "Hello, World!" > /dev/null. 
    * It's important to note that /dev contains "fileable" devices, mainly "static devices" (readable, writable and with persistent data), including "storage devices" such as hard drives (/dev/sda, /dev/sda1 for partitions), CD-ROM drives (/dev/sr0), character devices such as terminals (/dev/tty), pseudo devices (/dev/null, /dev/zero, /dev/random), audio devices (sound cards and audio input/output devices, e.g., /dev/snd/pcmC0D0p), network devices (/dev/net/tun for virtual network interfaces), and input devices (like keyboards and mice, /dev/input, /dev/input/mouse0). "Dynamic devices" (not readable, not writable, non-persistent data and frequently updated) differ from static devices; think of static devices as "the one need to be processed" and dynamic devices as "the one who processes". Dynamic devices, such as CPUs and memory, are usually hard to represent as files and appear as **virtual files** which we are going to talk about in next section, not placed in /dev. 

  * ## 2E. **/proc**(a virtual file)
    * Short for "process", it is a virtual directory providing dynamic information about the system's operation. It is mainly used to view kernel data structures and system process information, generated dynamically by the kernel and does not exist on any storage device. It displays the state of the kernel and processes through "virtual files" and "virtual directories". It also includes the status of the CPU and memory. 
    * These virtual files are also "dynamic files" (the data changes over time, so each time you open them, they might be different). 
    * Virtual files are not real files and do not have a "file system"; they are designed to look like a file system to facilitate user reading and operation. 
    * Most of the virtual files in /proc are plain text files, purely to display real-time process information (mostly read-only). They are generated by the kernel and placed in memory, meaning /proc is directly placed in memory, not on a hard drive! If you try to modify its content and save it using any text editor, an error will occur because the editor cannot modify files not on a storage device (hard drive). Modifications require special interfaces. 
	
  * ## 2F. **/sys**(also a virtual file) 
    * Short for "system". It is also a virtual directory providing dynamic information about the kernel's operation. Pretty similar as /proc. Here's an explanation I found:
      * Essentially /proc and /sys are the same. sysfs was added in kernel 2.5 or 2.6 due to clutter in procfs. The procfs was only meant to hold process information. eventually everything started getting mixed into proc and it created a twisty maze with device data stuck in different spots all over the place. Meanwhile, sysfs was implemented with the objective of segmenting device data from procfs. Specifically, /sys maintains more detailed (position of nodes actually represents the device hierarchy by subsystem) device process information. For each object in the driver model, a directory is created. The device file structure being:
        * **/sys/devices**: devices by physical layout
        * **/sys/bus**: symbolic links to devices
		* **/sys/block**: devices by block
		* **/sys/class**: devices by class
      * On your local system you might find man page at man sysfs and information about modifying kernel parameters in /proc/sys with man sysctl. 
      [Here's the source explanation talks about this](https://serverfault.com/questions/65261/linux-proc-sys-kernel-vs-sys-kernel)

# Chapter_3. Linux file system, ext4, inode
  * ## 3A. inode
  * ## 3B. ext4 

# Chapter_4. Memory Management Mechanism (Virtual Memory Mechanism; additional important content can be found in the C language notes on freestanding)
  * ## 3A. **Memory Fragmentation** 
    * One of the key issues that memory management aims to resolve is memory fragmentation, which refers to memory that cannot be utilized. It can be divided into "external fragmentation" and "internal fragmentation"([reference](https://www.nowcoder.com/discuss/544284985797169152))
      * **External Fragmentation**: Occurs because any application requires contiguous memory when requesting it. For example, suppose there are 16MB of physical memory with the following usage: |Unused (1MB)|App A (6MB)|Unused (2MB)|App B (3MB)|Unused (1MB)|App C (3MB)|. If App D requests 3MB of memory, the total available memory is sufficient, but none of the free memory blocks are large enough. 
      * **Internal Fragmentation**: To solve external fragmentation, a simple way is to divide memory into units of blocks, for example, 3MB per block, and allocate at least one block at a time. If App A needs 6MB, it gets 2 blocks; if App B needs 3MB, it gets 1 block. However, if App D needs only 1MB it will still be allocated one block (3MB), the extra 2MB is called "internal fragmentation" which is unused. This occurs when the total length of memory required by your application cannot perfectly divide the block size. 
  * ## 3B. Management Mechanism
    * Operating systems generally introduce a virtual memory mechanism to use memory more efficiently, which can be divided into segments or pages. 
      * **segment**(would have "external fragmentation"): Provides an application with a contiguous physical address. The starting address is marked as the "base address", and the range is recorded as the "segment boundary length". Every single memory within the segment has its own "segment offset" (its distance from starting address to current position, which is relative distance). The linear address is derived by adding the base address and the offset(which is basically physical address if there's other else conditions). The starting point of virtual memory within the segment is irrelevant as long as it's contiguous, though external fragmentation can occur.
      * **page**(with "internal fragmentation", used by Linux), The Linux kernel uses pages and page tables to implement virtual memory. Pages are units of virtual memory blocks (with continuous virtual addresses) mapped to contiguous physical memory blocks. For example, an application might use page1, page2, and page3, which are contiguous(application need gontiuous memory), but their corresponding physical memory blocks (e.g. block4, block7, block8) might not be. To minimize internal fragmentation, Linux uses mechanisms like the buddy allocator.      
        * **page tables**: Store the mappings of all virtual memory(pages) addresses to their corresponding physical memory(blocks) addresses. Pages tables themselve are also stored in memory.
        * **page**: A fixed-size unit of memory management. Most systems use 4KB page sizes, but there are others, some is smaller or some is larger like 2MB and 1GB (referred to as large pages or huge pages).
    * A **process** contains a program. Its memory usage layout is termed "process memory", usually composed of multiple pages. Each page only belongs to one process, mapping to a block of contiguous physical memory
	  * **Address Space** of process(important) 
        * Address space refers to the continuous range of virtual addresses a process covers from start to finish. The classic 32-bit address space ranges from 0xFFFFFFFF to 0x00000000, totaling 4GB (this is only address space for addressing, not actually being used).
        * This address space is further divided into **OS kernel space** and **User mode space**:
          * **OS kernel space** addressing: From 0xFFFFFFFF to 0xC0000000, totaling 1GB.
            * Kernel space is divided into |Direct map of all physical pages|VMALLOC|Persistent mapping|High-end mapping|. 
          * **User mode space** From 0xBFFFFFFF to 0x00000000, totaling 3GB.[reference](https://www.ptt.cc/bbs/C_and_CPP/M.1472449011.A.2A3.html), [reference_2](https://unix.stackexchange.com/questions/509607/how-a-64-bit-process-virtual-address-space-is-divided-in-linux), [reference_3](https://cg2010studio.com/2011/06/26/process-in-memory/)
            * The user mode space within a process is further divided into |High address|Stack|->Unoccupied address space<-|Heap||Data (global variables)||Text (code)|Low address|. 
              * The stack grows downward from the high address, dynamically encompassing declared variables and functions, following the stack data structure (although the size of a function and variable is static, declaring and destroying them during runtime causes the stack to grow and shrink). The heap grows upward, changing dynamically during runtime through system calls to adjust variable sizes, while data and text are fixed in size (they must be fixed to prevent the heap's lowest address from being forced to move). 
              * Note: "Unoccupied address space" is just address space, meaning the 3GB user mode space address space does not count "unoccupied address space" in actual memory usage. Thus, the memory size occupied by a process is dynamic, adhering to the principle of minimizing memory waste in management. 
        * These two modes relate to CPU privileges. In the x86 CPU's protected mode, different processor privileges are divided into four levels: Ring 0, 1, 2, and 3. Ring 0 has the highest privileges, allowing direct access to all hardware, while Ring 3 has the lowest privileges, with no hardware access. This design enhances fault tolerance, protecting data and the system from malicious software during errors. The OS kernel runs in Ring 0, device drivers use Rings 1 and 2, and applications run in Ring 3. Thus, the OS divides into kernel mode and user mode: kernel mode executes hardware access instructions, while user mode programs access resources through system calls to switch to kernel mode. 
		* Conclusion: In simple terms, a process's memory can divided into **user mode space** and **kernal space**. **user mode space** includes stack, heap, and segment, each composed of several pages(**Kernel space** is similar). Pages are the smallest unit of Linux memory management. [reference_1](https://hackmd.io/@sysprog/linux-memory), [reference_2](https://ithelp.ithome.com.tw/m/articles/10158922)

# Chapter_5. Process and Thread, explanation
  * A program is the logic implement to complete a computing task (which can be very complex). We write it using programming languages, then compile it into a series of CPU machine code instructions. These instructions are stored in memory and processed by the CPU in batches, ultimately solving the computational "tasks" that the program aims to accomplish. The earliest operating systems were created to handle the issue of "multi-task parallelism". Before the advent of operating systems, all computers were embedded systems, capable of running only one program at a time. Any program would occupy all hardware resources and physical memory (refer to the C language notes on freestanding and embedded system notes). To allow a single computer to perform multiple tasks concurrently, people created a large program called an "operating system", which is a program that manages other programs. One of the primary tasks of an operating system is to use virtual memory management logic to create "Processes". A process is a virtual memory space carrier for a program's machine code execution. From a physical standpoint, a process's virtual memory carries the machine code of a program, and the resources a program can use within a process are those allocated by the operating system to the process. 
  * Today, a computer still only runs one program, the operating system. However, this super program uses virtualization technology to carry many virtual programs, which operate similarly to ancient embedded systems. These virtual programs each only occupy the virtual resources (virtual memory) of a process and can request additional support functions (system calls) from the operating system. The operating system uses "time-sharing" to allocate CPU time to different processes (e.g., a short period for program A, then a short period for program B, and so forth), creating the illusion of parallelism.
  * As program designs became increasingly complex nowaday, it was discovered that many tasks within a single program also needed parallel processing. This led to the creation of threads. Essentially, threads are smaller tasks within the larger task(process). It’s important to note that these smaller tasks (threads) belong to the larger task (process), but the larger task itself is independent. A thread exists to serve the process it belongs to and to help complete it. All threads within a process serve that specific process, whereas processes in an operating system are independent of each other and exist to complete their own tasks. 
  * The operating system is designed to allow threads to share the resources of their parent process efficiently, which is a reasonable and effective design. 
  * Every process has at least one thread.[reference_1](https://www.reddit.com/r/explainlikeimfive/comments/35qw88/eli5_what_is_a_thread_in_a_processor/)

# Chapter_6. system call
