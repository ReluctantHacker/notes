|#############################################################################################################|
1. Recursive Algorithm(basic)
	A. Factorial
	B. Fibonacci Number
	C. Greatest Common Divisor(GCD)
	D. Binomial Coefficient
	E. Ackerman's Function
	F. Tower of Hanoi
	G. Permutation

	H. 關於recursion relation的closed form(solution), 在數學上很重要!!!!, some references below:
		https://brilliant.org/wiki/linear-recurrence-relations/
		https://www.eecs.yorku.ca/~tgeorge/courses/EECS1028W20/solving-rec.pdf

|#############################################################################################################|
2. 排序Sort
	A. 初等排序方法:  (平均average case: O(n^2))
		a. bubble sort 
			I. 時間複雜度(avg): O(n^2) 
			II. 時間複雜度(worst): O(n^2) 
			III. 時間複雜度(best): O(n) 
				🤔. 你每次比較一輪都可以偵測是否有裡面的元素有swap過, 如果完全沒有swap過, 表示已經排序好了, 最好裝況是一開始就已經排好, 你只需要比較第一輪完就能確定

			IIII. 穩定性(stability): stable
			IIIII. 空間複雜度: O(1)
			
		b. selection sort
			I. 時間複雜度(avg): O(n^2) 
			II. 時間複雜度(worst): O(n^2) 
			III. 時間複雜度(best): O(n^2) 
				🤔. 此法沒有機制可以確定在"過程中"就已經sort完了, 只能確保"最後"一定sort完
			IIII. 穩定性(stability): un-stable
			IIIII. 空間複雜度: O(1)
		
		c. insertion
			I. 時間複雜度(avg): O(n^2) 
			II. 時間複雜度(worst): O(n^2) 
			III. 時間複雜度(best): O(n) 
				🤔. 如果一開始就已經排好了, 每一輪都只需要比較一次(而且不需移動), 所以比n次即可
			IIII. 穩定性(stability): stable
			IIIII. 空間複雜度: O(1)

	B. 高等排序方法:  (平均average case: O(nlog(n))  
		z(補充). recursion tree(遞迴樹):
		reference:
		https://mycollegenotebook.medium.com/%E6%99%82%E9%96%93%E8%A4%87%E9%9B%9C%E5%BA%A6-%E9%81%9E%E8%BF%B4-%E4%B8%8A-f6d51a462394
		https://mycollegenotebook.medium.com/%e6%99%82%e9%96%93%e8%a4%87%e9%9b%9c%e5%ba%a6-%e9%81%9e%e8%bf%b4-%e4%b8%8b-master-th-307ad4608ab6		
			I. 以下會提到的高等排序法皆使用或間接使用"divid and conquer", 他們的平均時間複雜度為O(nlog(n)), 但why??
			II. 我們可以用recursion tree把這些排序法展開, recursion tree的高度為O(log(n)), 
			III. 每一層所需計算總次數為O(n) ---> (這個你仔細想, 他每一層事實上元素總量沒有變!!! 只是分成比較細而已, 你x層和x+1層要比較的次數是一樣的)

			
		a. quick sort  (個人稱之為"hi-lo-pivot sort", 因為使用到hi, lo, pivot作為遞迴時的變數)
			I. 時間複雜度(avg): O(nlog(n)) 
			II. 時間複雜度(worst): O(n^2) 
				🤔. 如果排序本身是完全反向的, 那妳每一輪pivot都會擺到最後(後最前看你怎麼設定), 所以你完全沒有"divide", 自然就退化成低等的排序法了
			III. 時間複雜度(best): O(nlog(n)) 
			IIII. 穩定性(stability): un-stable
			IIIII. 空間複雜度: O(log(n))
			IIIIII. 算法:
				🤔. 基本算法如下:
				reference:
				https://www.shubo.io/quick-sort/
					quickSort(): {
						partition()   // 找出pivot位置(此時pivot的元素已經確認其順序)
						quickSort()   // 0到pivot 做 quickSort
						quickSort()   // pivot到end做 quickSort
					}

				🤔. Partition如何處理, 有兩個常見的辦法:
					😄. Lomuto Partition Scheme (比hi小的元素都丟左邊)
						😄. 首先選出一個pivot, 這邊是用陣列內的最後一個元素arr[hi]
						😄. 用i紀錄下一個小於等於pivot的元素所要放置的位置, 初始化為lo
						😄. 接著遍歷 arr, 範圍從 lo 到 hi - 1, 當發現小於等於pivot的元素時, 就跟位於i的元素交換位置. 每次交換完, 就把i往前加一
						😄. 遍歷結束以後, 再把位於i的元素和位於hi的元素(也就是pivot)作交換
						😄. 最後回傳 i，它就是 pivot 的 index

					😄. Hoare partition scheme (hi-lo夾擠法)
						😄. 選擇陣列中央的元素作為pivot
						😄. 從最前面開始掃描大於pivot的元素, 從最後面開始掃描小於pivot 的元素, 找到之後交換
						😄. 重複以上步驟, 直到lo和hi相遇

		b. merge sort
			I. 時間複雜度(avg): O(nlog(n)) 
			II. 時間複雜度(worst): O(nlog(n)) 
				🤔. 
			III. 時間複雜度(best): O(nlog(n)) 
			IIII. 穩定性(stability): stable
			IIIII. 空間複雜度: O(n)
			IIIIII. 算法:
				🤔. 基本算法如下:
				reference:
				https://hackmd.io/@Aquamay/BylVMPFkiu
					mergeSort(): {
						mergeSort()   // pivot到end做 quickSort
						mergeSort()
						merge()
					}
				
				🤔. 你可以注意到這算法得function執行順序跟quickSort剛好相反!! quickSort是從father->child計算, mergeSort是從child到father計算!
				🤔. 你在仔細看會發現他跟binary tree的traversal八七相似, 因為他們原理其實大同小異!

		c. heap sort
		reference:
		https://stackoverflow.com/questions/9755721/how-can-building-a-heap-be-on-time-complexity
			I. 時間複雜度(avg): O(nlog(n)) 
			II. 時間複雜度(worst): O(nlog(n)) 
				🤔. 
			III. 時間複雜度(best): O(nlog(n)) 
			IIII. 穩定性(stability): un-stable
			IIIII. 空間複雜度: O(1)
			IIIIII. 算法:
				🤔. 基本算法如下:
					😄. 先把未排序的arr[]的數據按照heap的規則丟進一個heap, 這需要O(n)--->這個證明比較複雜, 有空再研究吧...
						😄. heap必為complete binary tree 且父node必大於子node(min heap則反之)
						😄. 插入node進heap
							😄. node先到complete B.T的最後, 然後向上挑戰父node, 反覆執行, 直到以下發生:
								😄. 挑戰失敗
								😄. 無父node
							😄. insert動作的時間複雜度為O(log(n)), 因為"往上挑戰父node的平均次數為樹高的一半, 其實就是log(n)/2, 


					😄. 再從heap的root一個個extract出來(這個時候就自動排序了)須O(nlog(n))
						😄. extract root from heap
							😄. heap的最後一個node移去補root的位置(之所以最後一個node, 是因為要維持complete B.T的結構), 但就這樣必定不是Heap了, 所以要做以下調整
							😄. 新root與自己的子node比大小, 最大的子node跟這root換位置, 一直往下比, 直到
								😄. 碰到底層, 
								😄. 或是已經沒有更大的子node了
							😄. extract動作的時間複雜度也為O(log(n)), 因為"往下挑戰品均次數也會為樹高的一半, 也就是log(n)/2
							😄. extract所有node所需時間複雜度為:(以下推倒方式有點問題...因為高度為2的節點不只一個)
								😄. log(1) + log(2) + log(3) + ... log(n) = log(n!) =~ nlog(n), 根據stirling formula  ----> O(nlog(n))
					
					😄. 最終結果時間複雜度還是O(nlog(n))

			????甚麼叫做sort是否in-place?

        d. Top-k問題
            I. 描述:
                🤔. 在算法领域，有一个经典的问题，用一句话就可以描述清楚：“从长度为N的无序数组中找出前k大的数。”这就是所谓的Top k问题。
            II. 經典解法:
                🤔.  从最容易想到的解法开始，我们首先考虑排序。对N个数排序，然后取前k个，即可达到目标。无论用归并排序、选择排序、快速排序还是其它任何类型的排序算法，这种解法的时间复杂度下界是O(nlog(n))
 ，因为这是任何排序算法的最低耗时。     
            III. 終極解法:
                🤔. BFPRT(median of medians), 它在最坏情况下的时间复杂度仍然是O(n)
            IV. 缺點:
                🤔. BFPRT時常被認為是不實用的, 這是因為, 實際執行上常常 比 quickselect 慢，因為 BFPRT 要額外花費分組、遞迴中位數等計算成本。對於大多數應用，quickselect 平均時間表現很好，因此 BFPRT 僅在需要確保最壞情況時間時才會被使用。(但也有文章提到其實實用上仍可能具有優勢: https://arxiv.org/pdf/1608.04852)
                🤔. 
            reference:
            https://zhuanlan.zhihu.com/p/291206708
|#############################################################################################################|
3. 搜索(Search)
	A. Search的分類:
		a. Internal Search vs External search
			I. Internal search
			II. External search
				🤔. 資料量大, 無法在記憶中處理, 需放入更大的儲存空間中進行(ex: disk)
				🤔. 例如:
					😄. m-way search tree
					😄. B-tree (平衡版的m-way search tree)
	
		b. Static vs Dynamic search
			I. Static:
				🤔. 搜索的資料集不變動(或不經常)
			II. Dynamic:
				🤔. 搜索的資料集時常變動

	B. 線性搜索(Linear Search)  --- time complexity: O(n)
		a. 有分2種:
			I. Non-Sentinel:
				for (int i = 0; i < length; i++) {
				    if (array[i] == elementToSearch) {
					return i; // I found the position of the element requested
				    }
				}

			II. Sentinel:
				// a的最後面插入elementToSearch元素(不然如果a裡面沒有elementToSearch, while會停不下來)
                a[length-1] = elementToSearch;
				while(a[i] != elementToSearch) {
    				i++;
				}

				🤔. Sentinel的方法少了"i<length"的判斷所需花費的時間, 因此在array很大的時候, 省下約一半的時間, 可以說是用一個位置的空間換取了倍數的時間.

	C. 二元搜索(Binary Search)  ----> 需要已經排列好
	reference:
	https://stackoverflow.com/questions/21586085/difference-between-binary-search-and-binary-search-tree

		a. 精神:
			Binary search其實就是在一棵binary search tree進行搜索(binary search通常會表述為在一個sorted的collection裡面進行search, 但其實兩者是等價的)

		b. 根據其精神, 很自然知道其time complexity: O(log(n))

		c. 算法:
			I. 以array的中間元素為基準點, array左右對切, 判斷在左還右, 然後recursive執行, 是一個由大到小的過程
			II. 算法大致長成以下:
				binSearch() {
					dosomething;
					binSearch(); // 左邊
					binSearch(); // 右邊
				}

	D. 內差法搜索(Interpolation Search)  ----> 需要已經排列好, 且必須要要有array的direct access機制才行
		a. 精神:
			I. 和數學上的代數內插法有異曲同工之妙, 內插法有線性和非線性, 但在此我們暫時都把已排序的資料的value當成線性遞增(或遞減)	
			II. a[i] - a[j] 應該要大致正比於 i - j    (都當成線性來看的話)
			III. 找到位置的內差值
			III. 若x為搜索值, 則第一輪內插位置k 應有關係為 (x-a[0])/(a[end]-a[0]) = (k-0)/(end-0)
			III. 找k後, 判斷k位置的元素比x大還小, 若較小則在k的左側中繼續用內插法找尋(若較大就右邊)

		b. 根據其精神, 其time complexity: O(log(n))

		c. 算法:
			I. 基本可以採用while迴圈


			

|#############################################################################################################|
3. hashing
reference:
https://www.zhihu.com/question/20820286
https://www.scholarhat.com/tutorial/datastructures/hashing-in-data-structures
	A. 定義:
		a. 將x值透過"hash function" 計算成 "hash address",
		b. 一個hash table(或hash map) 中 有B個bucket, 每一個bucket對應一個hash address
		c. 一個bucket有S個slot, 每一個slot存1筆資料
		d. 在對應hash address地址裡的bucket中存入x值(若無collision或overflow)

	B. 重要特性:
		a. hash table的搜索時間複雜度為O(1), 與資料量n無關, 其原理如下(個人想出的原理)
			I. 首先有一點要知道, hash的時間複雜度與"memory的定址速度"大有關係, 兩者依賴, 而其實memory定址的本質也是hashing.  memory定址, 其時間複雜度正是O(1), 也就是說memory的大小與memory定址所需時間無關, 為何會這樣? 這樣的結果有點違反常識和直覺, 但其實我們的生活中處處都在使用hashing, 只是我們不自知. 舉個例子, 新來的同學小美, 這天第一天上學, 要找班長小王辦一些手續, 一進教室卻傻眼了, 班上有1000人. 小美不認識班上任何人, 加上小美天性害羞, 臉皮薄, 只能一個一個詢問"你是不是小王?". 按照這情況, 小美所需要的平均詢問次數為1000/2=500次. 這時候, 另一個新來的學生小綠, 也來到了班上, 她臉皮厚, 見班上人多, 便直接大喊一聲"小王在哪?!". 小王聞聲前來. 小綠只詢問了1次! 而且這跟班上人數沒有任何關係, 班上即便有3000人, 小綠依舊只需要詢問一次. 小綠便是使用了hashing! hashing的本質就是廣播! 當然如果班上有兩個人叫小王, 那就會出現衝突, 也就是碰撞, 但這裡先不討論. 回到剛才的話提, hashing本質其實就是廣播, 我一次讓所有人都收到, 讓yes的人自己出來, false的人不要動, 在這裡我們都先假設訊號傳播速度為無限快. 還需注意一點, 雖然小綠只詢問了一次, 但是訊號是經過了"所有人", 所有人都要能收到訊號, 並判斷自己是不是小王, 所以總共"判斷了"1000次, 只是這1000次是同時進行的, 所以時間上只花費了一次的時間.  這樣的概念是否可以應用在"電路"中? 試想以下, 假設現在有1號到1000號的燈泡, 按照亂序串聯在一條銅線上, 每一個燈泡的開動電壓為自己的號碼乘上1V, 也就是說1號燈泡需要剛好1V的電壓才會亮, 2號燈泡需要2V的電壓才會亮, 3號需要3V...以此類推. 我們如果要找到877號的燈泡, 我們最保守的作法便是一個一個測量, 看哪個是877V的電壓, 這時間複雜度為O(n), 其中n為燈泡總數. 但是如果我們直接在銅線通上電壓877V, 那除了877號的燈泡會亮以外, 其他所有的燈泡都不會亮, 我們直接便找到了877號燈泡! 這便是hashing, 也就是廣播, 我把每一個人都用一個代號(或是說是名字, 或ID)給她一個身分, 需要找這個人, 直接喊她的身分.

		b. hashing 不可逆, 你無法從結果推回原本的帶入值, 這需要hash function的特性配合才辦的到
		
		c. hashing 會把任意大小的数据转换成固定长度的数据

		d. hashing 必定有碰撞(collision), 因为哈希算法的输入域是一个无限集合, 而值域是一个有限集合, 将无限集合映射到有限集合, 肯定是会冲突, 重叠的

		e. 越是理想的hashing, 其碰撞概率越低

		f. load factor(附載因子), 表示hash map中已存储元素数目与hash map大小之比: a = n/m
			I. load factor越大表示填充程度越大, 你填入新元素collision可能性提高
	
	
	C. 簡單的hash function(這function必須無法回推, 即使知道function也無法回推, 除非用暴力法, 一個一個比對, 這招又稱作彩虹表, 無法回推即是指回推出來的結果有接近無限種或是非常多種可能)
		a. Middle squre(平方值取中間位數)
			I. 例如87^2=7569, 那便取56作為hashing address
		b. Mod (餘數，或 Division)
			I. 例如87 mod 7= 3, 那便取3作為hashing address
		c. Folding Addition (折疊相加)
			I. 將資料鍵值切成幾個相同大小的片段, 然後將, 這些片段相加, 其總和即為Hashing Addres
		d. Digits Analysis (位數值分析)
			I. 例如8723426, 選擇第3位和第5位(要選哪幾位看這些資料的位數是否足夠隨機), 即是24

	D. 碰撞(collision)或是Overflow處理(bucket中的slots滿了), 你overflow也必定collision, 所以兩個可以一起處理
		a. open address(開放定址)
			I. Linear Probing(線性探測)
				🤔. 若此bucket發生碰撞, 便把值往下一個bucket填入(也就是H(x)+i)), 依此類推, 直到沒有bucket可以填
				🤔. 這樣有個重大缺點, 就是容易形成資料群聚(Clustering)現象, 增加searching time

			II. Quadratic Probing(平方探測)
				🤔. 若此bucket發生碰撞, 則填入 H(x)+i^2 mod b, 其中b為bucket總數量
				🤔. 仍有clustering

			III. double hashing(雙重hashing)
				🤔. 若沒有碰撞, 使用H_1(x), 若有則使用(H_1(x)+i*H_2(x)) mod b
				🤔. 大幅度減少clustering
				🤔. H_2()應該要跟H_1()互相獨立, 
				🤔. It should never yield an index of zero

		c. Rehashing(再雜奏)
			I. 提供一系列的Hashing Functions: f1, f2, f3, … fn。若使用f1 發生overflow，則改用f2; 以此類推，直到 沒有overflow發生 或 全部function用盡.

		d. link list(鏈接串列, 或chain)
			I. 将哈希表中的每个槽（bucket）视为一个链表的头节点，发生碰撞时，将新的键值对添加到对应槽的链表中。

	E. hashing有時會用來儲存密碼, 因為這樣資料庫裏面的密碼是被hashing過的, 即使是管理人員也不曉得真實密碼的, 增加了安全性, 但是缺點就是, 你忘記了密碼的話, 就真的沒辦法, 只能透過第三方認證來救.


|#############################################################################################################|
3. Greedy Approach(GA) and Dynamic Programming(DP)  ---無向圖 等於 雙向的有向圖(無向圖的邊可以表示來回雙向), 所以以下的圖論問題, 都適用於有向或無向
reference:
https://www.zhihu.com/question/23995189
https://ithelp.ithome.com.tw/articles/10338991
https://ithelp.ithome.com.tw/articles/10277276
	A. GA和DP的本質:
		a. GA選擇當下的最佳解, 不考慮之前和之後的狀況, 他是階段性選擇最佳解, 例如:
			I. 假設當前有A, B, C三條路, 走A拿100$, 走B拿50$, 走C拿10$, 根據GA, 走A
			II. 以上有個問題, 如果走A之後的路, 選擇都只有1$, 2$ 3$, 而走C之後的選擇都有100$以上, GA的結果便有可能不會是整體最佳解!

		b. DP選擇當前位置到達終點位置的最佳解, 他每一步都考慮了之後到終點的狀況, 例如:
			I. 假設當前有A, B, C三條路, 走A拿100$, 走B拿50$, 走C拿10$, 根據DP, 他要考慮A, B, C, 三條路到終點分別"最多"可以拿多少錢, 選擇拿最多錢的那條路. 若為A, A接下來又有三條路D, E, F, 則再用一樣的方法, 以此類推	
			II. 我們可以用數學, 整理出一個結論:
				🤔. f(當前位置) = max(f(A), f(B), f(C)), 其中f(當前位置)表示當前位置到終點"最多"可以拿多少錢, 他是一個典型的遞迴問題, 不斷解決子問題, 只是這個遞迴與之前遇到的其他單純算數不太一樣, 他是"選擇型"問題, 用max(), 或min(), 也有可能是其他選擇的方式來進行每一個階段的運算, 他的結果更佳, 但是跟GA比起來卻耗費很多計算, 因此斟酌使用


	B. Greedy Approach經典例子:
		a. 最小生成樹(minimum spanning tree):
		reference:
		詳細參考wikipedia, 以下都是整理自其中, 他講的挺詳細的
			I. 定義: 
				🤔. 其實"樹(tree)"是"無向圖(graph)"的一種, 只要沒有環的聯通圖就是樹了. 而最小生成樹其實就是, 再1無向圖中,  在所有的V都有經過(或者是說採到)的情況下, 權重總和最短的"最短連通子圖"(同時也是一棵樹) .

			II. 性質:
				🤔. 唯一性:
					😄. 如果圖的每一條edge的權值都不同, 那麼其最小生成樹唯一確定, 
				🤔. 存在個數:
					😄. 如果圖裡的權值全部相同, 則該圖的所有生成樹都是最小生成樹
				🤔. 環定理:
					😄. 對於連通圖中的任意一個環C：如果C中有邊e的權值大於該環中任意一個其它的邊的權值，那麼這個邊不會是最小生成樹中的邊
				🤔. 割定理: 
					😄. 在一幅連通加權無向圖中，給定任意的割，如有一條割邊的權值嚴格小於所有其他割邊，則這條邊必然屬於圖的最小生成樹。
				🤔. 最小權值邊:
					😄. 如果圖的具有最小權值的邊只有一條，那麼這條邊包含在任意一個最小生成樹中。

			II. 算法:
				🤔. Prim's 最小生成树算法（Prim's algorithm for Minimum Spanning Tree）
					😄. 輸入：一個加權連通圖，其中頂點集合為V, 邊集合為E
					😄. 初始化: V_new = {x}, 其中x為集合V中的任意節點(起始點), E_new = {}
					😄. 重複下列操作, 直到V_new = V
						😄. 在集合E中選取權值最小的邊(u, v), 其中u為集合V_new中的元素, v為V中沒有加入V_new的元素(如果存在多條邊滿足條件, 則可任選其一)
						😄. 將v加入集合V_new中, 將(u, v)加入E_new中
					😄. 輸出: 使用集合V_new, E_new表示最小生成樹
					😄. 時間複雜度:
						😄. 鄰接矩陣(adjacency matrix):
							😄. O(|V|^2)
						😄. 二叉堆(binary heap)
							😄. O(|E|log|V|)
						😄. Fibonnaci heap
							😄. O(|E|+|V|log|V|) ---> prim算法的最佳

				🤔. Kruskal 最小生成树算法（Kruskal's algorithm for Minimum Spanning Tree）
					😄. 新建圖G, G中擁有原圖中所有Vertex, 但沒有Edge
					😄. 將原圖的Edge, 按照權值有小到大排序
					😄. 從權值最小的Edge開始加入G中, 但若會形成環則不加入該邊
					😄. 重複上個步驟直到G有V-1個邊, G的結果即為最小生成樹
					😄. 時間複雜度:
						😄. 平均時間複雜度: O(|E|log|V|)

			III. 應用:
				🤔. 電路布局最小成本
				🤔. 連接n個城市之交通連線之最小成本
				🤔. 旅遊n個城市之最少花費(不回原點)		

		c. Dijkstra 单源最短路径算法（Dijkstra's algorithm for Single Source Shortest Path）
		reference:
		https://www.freecodecamp.org/chinese/news/dijkstras-shortest-path-algorithm-visual-introduction/
		https://zhuanlan.zhihu.com/p/129373740
			I. "單源最短路徑"的定義:
				🤔. 在加權圖中, 起點A的"單源最短路徑(以下簡稱SSSP)"為A到"其他所有點"的最短路徑的總圖, 這定義有點搞, 他的意思其實就是, SSSP包含的所有點, 到原點A的距離, 必定都會是此加權圖的最短路徑, 例如總共有A, B, C, D四個點, SSSP是指, SSSP上, A到B會最短, A到C也會最短, A到D也會最短. 在加權圖中, 你A到B可能好幾個路徑, 但只要它包含在SSSP之中, 那他必定是最短的. 注意!! 你要找的不是起點A到某點的最短路徑, 而是A點到所有點都是必須最短, 這個過程有機會跳來跳去, 可能不是連續的, 這是正常的!

			II. 算法:
				🤔. 確認原點和已經訪問過的點所成之集合V_f, 找到V_f的邊界點(他的節點與V_f相連但未訪問過)
				🤔. 這些邊界點的節點到原點的距離各自全部算出
				🤔. 從中尋找最小的, min(), 並將該節點作為新的已訪問節點加入V_f中
				🤔. 重複以上步驟, 直到沒有節點可以訪問
				🤔. 限制條件: 權重必須為正(Dijkstra不能處理negative權值問題)
				🤔. 時間複雜度: O(n^2), n為節點個數, 若是使用優化的數據結構(例如binary heap, Fibonacci heap), 時間複雜度還是可以再將低的

			III. 應用:
				🤔. 動態路由中有不少協議應用此種算法, 例如OSPF(使用很廣泛), ISIS等等

			IV. 算法限制:
				🤔. 必須邊長(權重)不可以是負值, 其他情況下可以保證最短路徑成立! 因為存在負權邊時，路徑可能會陷入無限循環中，導致無法確定最短路徑


		d. 贪心背包算法（Greedy Knapsack Algorithm）
			I. 背包問題定義:
				🤔. 給定一組物品，每種物品都有自己的重量和價格，若我們的背包有限定的總重量內，我們如何選擇, 把物品放入背包，使得獲得的總價格最高, 背包也不會超過總重量?

			II. Greedy算法:
				🤔. Greedy方法就是沒次都把最貴的東西塞進背包裡(或者是價格/重量 cp值最高的), 但是這樣的方法很可能不會是最佳解, 要更準的需得用動態規劃(但是更耗費算力)
				
		e. 最小硬币找零问题, 又稱"最少硬幣問題"（Minimum Coin Change Problem）
			I. 問題定義:
				🤔. 擁有x, y, z的幣值, 如何用最少的鈔票數(或是硬幣數)湊出T元
			II. Greedy算法:
				🤔. Greedy方法其實就是每次都選幣值最大的鈔票(或硬幣), 但是這樣當然不見得是整體最佳解, 所以要更好的結果, 還得要動態規劃
	
	B. Dynamic programming經典例子:
		a. Floyd-Warshall 全源最短路径算法（Floyd-Warshall algorithm for All Pairs Shortest Path）...感覺跟dynamic 沒什麼關係...
		reference
		https://ithelp.ithome.com.tw/articles/10209186
		https://www.geeksforgeeks.org/floyd-warshall-algorithm-dp-16/
		https://ithelp.ithome.com.tw/articles/10271531
			I. 全源最短路徑, 問題定義:
				🤔. 求任意兩點之間的最短距離(其結果要包含"所有"兩點的最短距離, 所以他會是一個矩陣)
			II. 算法:
				🤔. 先把圖的權重存入鄰接矩陣(adjacency matrix), 他會是一個2*2的矩陣, 如果兩點間沒有link則其值先用infinity代替
				🤔. 接著矩陣裡的每一個元素, 按照以下規則更新:
				reference:
				https://ithelp.ithome.com.tw/articles/10271531
				算法有點複雜, 參考wikipedia
				https://zh.wikipedia.org/zh-tw/Floyd-Warshall%E7%AE%97%E6%B3%95
					😄. 從元素i經過元素k, 到達元素j, 若"經過元素k到達j的距離"比"不經過k到達j的距離"近, 則將此距離更新上去, 否則維持
					😄. 當元素j遍歷完後, k++, 然後繼續同樣的動作, 直到k遍歷結束
					😄. 中心思想可簡化為以下:(詳細可參考wiki)
						😄. if(dist[i][j] > dist[i][k] + dist[k][j]) {
							dist[i][j] ← dist[i][k] + dist[k][j]
							}
					😄. 如此一來矩陣的結果便紀錄了, 所有點之間互相的最小距離
				😄. 限制條件:
					😄. 不能有negative權重環路(就是整個環路加起來為negative):
						😄. 在這樣的情況下，最短路徑的定義就變得模糊，因為可以無限次地繞行該循環，使得路徑長度無限接近負無窮。因此，對於包含負權邊但無負權環的圖，Floyd-Warshall算法可以正確地找到所有節點對之間的最短路徑；但如果圖中存在負權環，則最短路徑的概念就不再適用了。在Floyd-Warshall算法中，通常會對這樣的情況做出特別處理或者報錯。
				🤔. 時間複雜度: O(n^3)  ---->其實很容易理解, 矩陣本身就已經是n^2, 每一個元素又要尋找最短路徑, 花費O(n), 所以結果自然就是O(n^3)

		b. 背包问题（Knapsack Problem）
				🤔. 擁有x, y, z的幣值, 如何用最少的鈔票數(或是硬幣數)湊出T元 --> 與greedy一樣的問題

		c. 最长公共子序列问题（Longest Common Subsequence Problem）
		d. 编辑距离问题（Edit Distance Problem）
			I. 問題定義:
				🤔. 将一个字符串转换成另一个字符串所需的最小操作次数, 操作包含以下3個動作: 替換字符, 插入字符, 刪除字符

		e. 最长递增子序列问题（Longest Increasing Subsequence Problem）

		f. Bellman-Fold Algorithm算法 同樣是求"單源最短路徑"的算法(與Dijkstra算法處理一樣的問題) 
		reference:
		https://my.eng.utah.edu/~cs5480/notes/chapter4-2007-Part2.pdf
		https://www.youtube.com/watch?v=hdpnoOcrGck
			I. 算法:
				🤔. Define that ... d_x(y):=cost of least-cost path from x to y, then
				🤔. d_x(y) = min_v{c(x, v) + d_v(y)}, 其中c(x, v)表示x到v的直接距離(沒有經過其他), min_v() 會遍歷所有與x相鄰的v
				🤔. 時間複雜度: O(|V|*|E|)

			II. 優點和缺點:
				🤔. 可以計算negative權值
				🤔. 計算較費時
		
			II. 應用: 在RIP動態路由, DV 算法, 有所應用
