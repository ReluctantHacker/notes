|#############################################################################################################|
1. some datastructure
	A. primitive: integer, float, string, boolean
	B. non-primitive: 
		a. 線性(in sequential order): list, tuple, array, linked list, stack, queue
		b. 非線性: set, dictionary, tree, graph

|#############################################################################################################|
2. Program Complexity
	A. Space complexity

	B. Time complexity
		a. f ∈ o(g)  ---   order(f)<order(g)  ---  lim f(x)/g(x) = 0 , x->∞
		a. f ∈ O(g)  ---   order(f)≤order(g)  ---  lim f(x)/g(x) < ∞ , x->∞
		a. f ∈ ϴ(g)  ---   order(f)=order(g)  ---  lim f(x)/g(x) ∈ R > 0 , x->∞
		a. f ∈ Ω(g)  ---   order(f)≥order(g)  ---  lim f(x)/g(x) > 0 , x->∞
		a. f ∈ ω(g)  ---   order(f)>order(g)  ---  lim f(x)/g(x) = ∞ , x->∞

|#############################################################################################################|
3. Recursive Algorithm(basic), 遞迴問題的closed form(公式解, 請參考algorithm.txt筆記)
	A. Factorial
        a. 一般正常的factorial recursive版本很簡單, 但是有種迴圈概念叫做"tail recursive", 這不是只針對factorial的用法, 所有的遞迴都有機會實現(我只是方便講, 所以放在開頭的factorial一起說). 一般的recursion函數呼叫, 通常呼叫完後還會做一些事情, 所以就會發生從一開始迴圈開頭進入到深層迴圈之後, 又從深層跑回淺層把尚未完成的工作完成, 簡單來說就是stack的概念, stack保留了一開始的東西, 完成深層之後再返回到淺層, 也就是他實際上"來回". 而tail recursive就是希望只有"來"或者"回". 從數學上來講, tail-recursive不會比較快, 但是物理上來講他會比較節省記憶體, 因為不需要stack了, 雖然是遞迴, 但是在下次呼叫recursive之前已經結束當前函數的所有工作, 所以不需要stack去存(變成stack永遠只需要一個level就夠了), 物理結構上已經有點向for迴圈了. 但需要注意的是, 許多語言的編譯器(例如c語言的gcc), 不會特地去幫你優化tail recursive, 也就是說他不會因為你是tail recursive就幫你把不用的stack省出來, 而是依舊會堆疊上去, 所以記憶體沒有比較省. (並非所有recursive都能化為tail)
            I. non-tail recursive version of recursive factorial:
                🤔. int factorialRec(int n) {
                        if (n==1 || n==0) return 1;
                        return factorialRec(n-1)*n; // 這裡可以看到factorialRec(n-1)呼叫之後(先進入函數下一層了)還乘上n, 所以stack要把未處理完的事情記下來
                    }
            II. tail recursive version of recursive factorial:
                🤔. int factorial_tail(int n, int acc) {
                        if (n == 0) return acc;
                        return factorial_tail(n - 1, acc * n); // Tail call 這裡呼叫完沒有做其他事情了, stack不需要保留未處理完的事情
                    }


	B. Fibonacci Number
	C. Greatest Common Divisor(GCD)
	D. Binomial Coefficient
        a. C(5, 3) = C(4, 2) + C(4, 3)
        b. 甲乙丙丁戊選3人組隊, 幾種組法?
            I. 其實背後的直觀解釋就是若甲必定是3人中的一員的祖法數加上甲必定不是3人中一員的組法數等於五人中選三人組法數
	E. Ackerman's Function
        a. 需要提到一個概念叫做"原始遞迴函數(primitive recursive functions)", 符合以下兩個規則(vector_x是指很多變數x1, x2, x3, x4...任意數量, 推進遞迴的參數只有y, 相當於最簡單的遞迴中的n變數. 我們一般遇到的遞迴都沒有x(或者說x是常數), 只有y(也就是n), 實際上較為複雜的遞迴也會有x.(其實我們遇到大部分的遞迴問題都是原始遞迴, 例如之前提到的所有遞迴問題, factorial, fibonacci等等)
            I. f(vector_x, 0) = g(vector_x)
            II. f(vector_x, y+1) = h(vector_x, y, f(vector_x, y)) 
            III. 範例(加法遞迴), 這個實際上就是第y+1項等於y項+1:
                🤔. add(x, 0) = x
                🤔. add(x, y+1) = add(x, y) + 1    ---->單一遞迴, 只有y有貢獻遞迴效果
            IV. 範例(乘法遞迴)
                🤔. mul(x, 0) = 0
                🤔. mul(x, y+1) = add(x, mul(x, y))
            V. 誤區1: 剛才提到遞迴項y+1, 有個誤區是假設奇數項1, 3, 5, 7之間互相遞迴, 偶數項2, 4, 6, 8, 互相遞迴, 這樣整個數列還算是單一遞迴? 其實這樣就變成兩個無關的單一遞迴, 只是混再一起而已. 更精確地來說即使是y+2, 你仍然可以將y+2化成z+1的形式, 反正項與項之間的差距本來就是相對的, 假設我們只能透過某個遞迴函數得到偶數項, 那實際上來說偶數項就是所有的項了. 原始遞迴(單一遞迴)的核心不是要求"一定要逐項+1"去算, 而是"能否將其化成逐項", 如果能, 就還是單一遞迴. 更進一步說, 遞迴參數必須是線性例如ay+b, 其中a和b都是常數, 這樣必定能化為z+1, 若為非線性(例如y^2有關或者y=m+n的兩個參數依賴), 其不是原始遞迴.
                🤔. f(x,0)=g(x),f(x,y^2)=h(x,y,f(x,y)) ----> 不是原始遞迴
                🤔. 有個概念叫做"線性遞迴", 與剛才提到的遞迴參數成線性的概念有點像, 但兩者指的不是一回事. 線性遞迴實際上是指a_n = c1*a_(n-1) + c2*a_(n-2) + ...ck*a_(n-k) + f(n), c全部都是常數, 也就是第n項是前面項的線性組合, 更直接來說就是h()是線性函數
            VI. 誤區2: 剛才提到的遞迴參數為y^2的時候, 他不是原始遞迴, 但他也不是non-primitive recursive(非原始的遞迴), 區別在於y^2本身不是"total"的, 及所有整數項都涵蓋, 他是1, 4, 9, 16, 25...項. 而我們說的non-primitive recursive必須是total的(non-primitive recursive仍舊是遞迴, 他不是非遞迴), 我們甚至可以說y^2的遞迴參數本身連遞迴都不算.
        b. 要徹底理解ackerman function在做甚麼, 需要介紹一個數學領域叫做"計算理論(theory of computation)"
            I. 採用甚麼計算模型(形式語言, 自動機)
            II. 那些問題可計算, 哪些不可計算(可計算性理論computability theory, 演算法)
                🤔. 以下有三個等價定義:
                    😂. 存在一個圖靈機(或等價機)M, 對任意輸入x, M在有限步驟後停機, 並輸出f(x)
                    😂. μ-遞迴函數定義(μ-recursive functions, 也稱作general recurisve functions)
                        😂. f 屬於由以下基礎函數透過組合（composition）、原始遞迴（primitive recursion）、μ-算子（最小化算子）所生成的集合：
                            😂. 零函數Z(x) = 0, 零函數是常數函數f(x) = C的特例
                            😂. successor function後繼函數S(x) = x+1, 其實就是輸入一個自然數, 輸出其下一個自然數, 看起來像廢話, 但其實它有用在有名的"定義自然數的皮亞諾公理"中, 用以定義自然數的order和結構
                                👍. 皮亞若的5個公理(自然數的公理系統, 以下是非形式化論述):
                                    👌. 0是自然數；
                                    👌. 每一個確定的自然數a，都有一個確定的後繼數S(a)=a', a' 也是自然數；這裡S()是後繼函數, 注意這裡"尚未"定義S(x) = x+1, S()只表示後繼而已, 沒有給出運算
                                    👌. 對於每個自然數b、c，b=c若且唯若b的後繼數=c的後繼數；
                                    👌. 0不是任何自然數的後繼數；
                                    👌. 任意關於自然數的命題，如果證明：它對自然數0是真的，且假定它對自然數a為真時，可以證明對a' 也真。那麼，命題對所有自然數都真。
u
                                👍. peano公理只定義了order, 起點, 唯一性, 歸納性質, 並沒有運算性質, 運算性質是由peano Arithmetic(PA)定義, 其實只是peano公里加上加法公里and乘法公理
                                    👌. 加法公理: 
                                        🥵. x+0=x,
                                        🥵. x+S(y)=S(x+y)
                                    👌. 乘法公理: 
                                        🥵. x⋅0=0,
                                        🥵. x⋅S(y)=x⋅y+x 
                            😂. 投影函數U_i^x(x1, x2..., xn) = xi, 概念非常之簡單, 他與幾何學中的投影求分量有一點點類似但更加廢話, 他其實就是輸入很多值x1, x2..., xn, 函數會輸出某個輸入值, 例如x2, 就只是挑出一個., 例如P^3_2(x1, x2, x3) = x2
                        😂. μ-遞迴中的μ-算子(原始遞迴前面已經提過, 這裡跳過), 有了μ-算子後(加上前面的條件), 即是μ-遞迴函數(包含所有圖靈可計算函數), 簡單來說"原始遞迴函數"+"μ算子"="μ遞迴函數"(大抵上), 與圖靈可計算函數等價.
                            😂. 找最小且符合不等式(或等式)條件的y(也就是前面提到的遞迴參數), 例如:
                                👍. f()=μy[10−y2≤0]=4 
                            😂. 直覺就是從0開始逐個檢查, 找到第一個滿足條件的y
                            😂. μ-算子讓函數可以「無限搜尋直到找到解」，這是原始遞歸無法做到的，因此 μ-遞迴函數集合更大，與圖靈可計算函數等價。
                            😂. 更直觀和精確的來說, 原始遞迴函數g(y), 即輸入y得出g(y), 而μ-算子則是利用g(y), 他實際上就是y=0, y=1, y=2...y=n帶入g(y)直到g(y)到達某個值而停下, 這個操作本身就μ-算子, 而這個操作無法由原始遞迴函數本身自己建構出來, 他是一個獨立運算子, 所以"擴展"了原始遞迴函數, 這個"擴展"的概念在數學上的其他地方隨處可見
                            😂. 需要注意的是原始遞迴本身也可以用μ-算子表達! 這裡別誤解了, 如果你設計的條件 剛好只有一個解，而且立刻在某個確定的 y 就出現，那 μ 算子就等同於直接「返回那個確定值」。在這種情況下，它就退化成一個原始遞歸函數的寫法。簡單來說μ算子是non-primitive recursive的必要條件, 但μ算子成立不代表其必定是non-primitive recursive(也因此你不能簡單說只要證明ackermann function能用μ算子表達則他就是non-primive recursive)

                    😂. λ-演算定義 (λ-definability), f可以在λ-演算裡被定義  
                🤔. 直觀來說, 有以下兩點:
                    😂. 有限步驟
                    😂. 步驟明確(即演算法)
            III. 計算複雜性理論, 其實就是那些甚麼大O理論之類的
        c. Knuth’s up-arrow notation(是一種hyperoperation, 詳細可參考en.wikipedia, 只使用在有限數, 無限理論不適用, 因為該符號不會讓數超越該finite realm)
            I. the single arrow ↑ represents exponentiation (iterated multiplication)
                🤔. 2↑4 = H_3(2, 4) = 2x(2x(2x2)) = 2^4 = 16
            II. the double arrow ↑↑ represents tetration (iterated expoentiation)
                🤔. 2↑↑4 = H_4(2, 4) = 2↑(2↑(2↑2)) = 2^16 = 65536
            III. the triple arrow ↑↑↑ represents pentation (iterated tetration)
                🤔. 2↑↑↑4 = H_5(2, 4) = 2↑↑(2↑↑(2↑↑2))

        d. Ackermann函數, 沒有直接使用μ算子, 但仍可以透過μ算子建構, 因為μ算子可以用來模擬間接遞迴. 要證明ackermann函數為non-primitive的話, 策略就是: 證明沒有任何原始遞迴函數是ackermann函數. 也可以證明任意primitive函數的漲勢必定被Ackermann壓住, 有點類似我們討論數列發散的時候的漲勢(有些數列的漲勢會壓制某些數列)的那種情況, 這裡有一些關於證明的討論[https://math.stackexchange.com/questions/1228666/the-ackermanns-function-grows-faster-than-any-primitive-recursive-function]
        e. Ackermann函數的性質, 與超越性的證明:
            I. 基本上m每+1, 增長速度生一個層級, 可以當成n是序列增長, m則是序列階級增長了
                🤔. A(0, 0) = n+1
                🤔. A(1, n) = 2+(n+3)-3
                🤔. A(2, n) = 2*(n+3)-3
                🤔. A(3, n) = 2↑(n+3)-3
                🤔. A(4, n) = 2↑↑(n+3)-3
                🤔. A(5, n) = 2↑↑↑(n+3)-3
                🤔. A(6, n) = 2↑↑↑↑(n+3)-3
                🤔. A(m, n) = 2(↑^(m-2))(n+3)-3
            II. Grzegorczyk hierarchy(primitive recursive 的增速層級), 這東西是原始遞迴的速度分級
            III. 每個PR都被固定在某個A(m, )的速率之中, 但Ackermann本身m是個變數, m本身加一速率往上跳一個階級. 所以可以得出由於ackermann的速率階級不是固定的, 而primtive的速率階級是固定的, 所以ackermann必定不是primitive
        f. 巢狀遞迴(nested recursion), 直白定義就是遞迴參數本身是遞迴函數的輸出(類似泛函數的概念, 及函數的函數), ackermann就是一種巢狀遞迴
            I. 基本定義和說明
                🤔. 普通遞迴: f(n) = g(f(n-1), n)
                    😂. 例如: f(n) = 2*f(n-1)+1
                🤔. 巢狀遞迴: f(n) = g(f(f(n-1), f(n-1), n)
                    😂. 例如: f(n) = 2*f(f(n-1))+f(n-1)+1
            II. "巢狀迴圈(nested loop)"是甚麼, 名詞很像?
                🤔, 巢狀迴圈其實就是for裡面在一個for, 就是寫代碼很常見的多層迴圈而已, 只是給這種情形一個名詞而已
            III. 以下有個關於各種形式遞迴(線性遞迴, 二元遞迴,等等一堆有的沒的說明)
                🤔. https://medium.com/learn-or-die/recursion-%E9%81%9E%E8%BF%B4-%E4%B8%8D%E5%90%8C%E9%A1%9E%E5%9E%8B%E7%9A%84%E9%81%9E%E8%BF%B4-659ab2f53466
        
            
	F. Tower of Hanoi
        a. 解釋:
            I. 1-levels,  1
            II. 2-layels,  1*2 + 1 = 3 = 4-1 = 2^2-1
            III. 3-levels,  3*2 + 1 = 7 = 8-1 = 2^3-1
            IV. 4-levels, 7*2 + 1 = 15 = 16-1 = 2^4-1
            V. n-levels, 2^n-1
            VI. so H(n) = 2^n-1 
            VII. or H(n) = H(n-1)*2+1
            VIII. 原理基本上就是A柱最底層先不動, 把其他上面的先挪到其中B柱子(花費步驟H(n-1)), 然後再把最底層移動到C柱子(花費步驟1), 接著再把B柱子的所有移動到C柱子(花費步驟H(n-1)), 所以總共花費步驟2*H(n-1)+1

	G. Permutation
        a. 數學中的P(5, 3) = 5!/2!, 五個挑三個出來排列即是5*4*3=5!/2!, 因為是排列所以排列循序有差(combination沒差所以多除以3!, C(5, 3) = 5!/(2!*3!) )
        b. Combination的係數就是二項式係數, 前面提過遞迴的原理, 而permutation的遞迴規律是
            I. P(n, i) = P(n-1, i)+i*P(n-1, i-1)
            II. 它的原理與combination的原理基本差不多, 例如甲乙丙丁戊選3個出來排列, P(5, 3)=P(4, 3)+3*P(4, 2)實際上就是甲沒有被選中的排列數為P(4, 3), 和甲必定被選中的排列數為3*P(4, 2), 後者是先把甲踢出, 只選兩個所以是P(4, 2)然後再把甲加回來排列(這樣必定有甲), 所以是3*P(4, 2), 因為甲可以排在3個位置.

    H. Backtracking problem(the basic idea of backtracking is finding all the path of a decision tree, by going deeper from root to deeper leaf and hit the dead end and go back, the problem must be representable as a decision tree, even if you don't draw it explicitly)
        a. Permutation(所有可能排列列出, 或存取)
            I. backtracking of permutation很tricky, 
            II. 算法採用for迴圈是很自然, 因為for迴圈是從頭經歷到尾把所有元素輪一遍
            III. 詳細的原理筆記可以另外參考coding的筆記, 用coding的方式把算法講清楚
                🤔, used[]法(有回朔的動作), O(n!)
                    😂. 他的原理是, 假設有[a, b, c, d], 我需要去記哪個元素已經被填入了
                    😂. 首先a, b, c 照循序填入path, 當然這些元素全部被記錄已填入used[]裡面, 然後固定a, b, c, 把剩餘的填入(也就是最後的d)
                    😂. 把c拿掉(當然c也從used[]裡面false掉, 取消紀錄), 然後a, b固定, 然後在原本c的位置(也就是第三個位置)填入c的下一個元素(也就是d), 由於d已經被使用, 所以最後一個元素很自然填入c
                🤔. swap()法(有回朔的動作), O(n!) 
            IV. Permuation還有heap's algorithm法, 但這個方法不是backtracking, 因為他沒有回朔的動作, 而是直接利用產出的permutation去組合下一個可能組合, 不會重複, 也因此swap的次數較少, complexity, O(n!)
            V. Lehmer code(給每一個排列unique編碼, 其實就等於給一個permutation一個unique的id, 用來代表這個permutation):
                🤔. 編碼規則:
                    😂. For each position, how many elements to the right are smaller than this one? (這個規則可以保證每一種permutation的編碼是不重複的)
                🤔. 性質:
                    😂. factorial numbers system(n=3的排列範例與其對應的lehmer code和factorial value), Factorial Value Calculation那個欄位其實就是Cantor's expansion
                        | Permutation | Lehmer Code (L\[i]) | Factorial Value Calculation | Factorial Value (ID) |
| ----------- | ------------------- | --------------------------- | -------------------- |
| `[1,2,3]`   | `[0,0,0]`           | `0*2! + 0*1! + 0*0!`        | `0`                  |
| `[1,3,2]`   | `[0,1,0]`           | `0*2! + 1*1! + 0*0!`        | `1`                  |
| `[2,1,3]`   | `[1,0,0]`           | `1*2! + 0*1! + 0*0!`        | `2`                  |
| `[2,3,1]`   | `[1,1,0]`           | `1*2! + 1*1! + 0*0!`        | `3`                  |
| `[3,1,2]`   | `[2,0,0]`           | `2*2! + 0*1! + 0*0!`        | `4`                  |
| `[3,2,1]`   | `[2,1,0]`           | `2*2! + 1*1! + 0*0!`        | `5`                  |
                    😂. Lehmer code和factorial規則所生成的Factorial Value會為連續自然數(因此不重複), 這是他的數學性質, 很剛好可以用來作為permutation的編碼(Lehmer code本身對一個排列也是唯一的)
                    😂. 當然存在其他編碼方式, 但Almost all ranking/unranking algorithms are either directly Lehmer code or a slight variation of it. It's the most famous and widely used method for assigning a unique ID to each permutation
                    😂. Lehmer code需要排列的元素有序(order), 例如自然數有序, 英文字母有序等等, 如果排列的東西沒有序, 你必須自己給那些元素定義序才能使用Lehmer code
                    😂. Lehmer code的結構簡單高效(efficient), 同時還是reversible(可以從factorial value 唯一逆向推回Lehmer code)
                
                
        b. combination(所有組合可能列出, 或存取)
        c. N-Queens
        d. Sudoku Solver
        e. Maze Solving

	I. Recursive算法的大原則如下: 
	method_A: 由child到father進行運算, 也就是由所有leaf一路算回root (例子: binary tree的所有相關算法幾乎都是)
		def recursiveFunc():
			recursiveFunc()
			doSomething()

	method_B: 由father到child進行運算, 也就是由root一路算到所有leaf (例子: quickSort)
		def recursiveFunc():
			doSomething()
			recursiveFunc()
    J. Recursive functions都只有一個stack來回call, 即使用iterative的方式實作可能用到兩個stack來完成, 但recursive functions在底層呼叫都只有一個stack!! 這個概念很重要, 因為若recursive method只需一個stack則iterative methods必定可以用一個stack實現(但可能不明顯)

|#############################################################################################################|
4. array 
	A. 定義:
		a. array內的元素同type
		b. 元素間占用的記憶體是連續的
	B. 不同維度:
		a. first dimensional array
		b. second dimensional array
			I. 其實沒甚麼好講的, 有一點比較需要注意的是, c語言(其實大部分流行的語言包含c++, python, java, C#, Go, Rust)存儲array是採用"列為主(row-major, 就是你熟悉的)", 有些程序語言採用的"欄為主(column-major)", 以下有個比較清楚的說明:
			https://ithelp.ithome.com.tw/articles/10333963?sc=pt
            II. c語言2d-array, 記憶體是連續的, 也就是第一個row的結尾記憶體位置下一個記憶體位置就是第二個row的第一個元素. 這實際上是比較直觀的方式
            III. c語言實際上並沒有2d-array的資料結構, 而是使用array of array或者**arr(指標的指標)去simulate 2d array, 以下兩種辦法介紹
                🤔. int arr[ROWS][COLS];其變數名稱arr會decay成int (*)[COLS], 這並不是**arr, 他是一個pointer of an array, 實際上, 當你使用void testFunct(int (*arr)[Cols]) {}的時候, arr實際上就是每一個row的第一個元素的地址.
                🤔. 使用int **arr的時候, 我們會定義, malloc()的使用是獨立的, 第一次使用malloc所返回的值不一定會和第二次返回的值連續, 所以arr[1], arr[2], arr[3]...不一定連續. 一下是c語言範例, 注意**arr的二維array模擬, 具有每個row不一定需要等長的自由特性, 且由於使用dynamic memory, 其每row大小可以自由調整
                    🤔. int **arr = (int **)malloc(RowNum*sizeof(int *));
                    🤔. for (int i=0; i<RowNum; i++) {
                            arr[i] = (int *)malloc(ColNum*sizeof(int));
                        }

		c. third dimensional array

|#############################################################################################################|
5. linked list
	A. 優點
		a. 動態性: 動態增加空間, 不需要一開始設定大小
		b. 非連續性: 不需要連續的儲存空間
		c. 在linked list任意位置插入元素, 或刪除元素無須將每一個元素移動(但注意, 你沒有該位置的位址, 仍要遍歷linked list才能訪問該元素, array則可直接藉由位置訪位該元素) : 時間複雜度O(1)(這是不算遍歷的)

	B. 缺點:
		a. 隨機訪問效率低: 需要從頭查找, 時間複雜度O(n), 若是array, 則只要指定位置即能直接訪問
		b. 占用額外的空間
	C. 補充:
		a. linked list其實可以看做是one branch tree, 也就是每個node都只有一個分支的tree
		b. tree的基本單位是node, linked list也是
		c. linked list與array是電腦裡面最基本的兩個資料結構, 兩者建構了其他的資料結構(在電腦實務科學上是這樣的, 因為記憶體的問題)
	D. Doubly linked list
		a. node裡面有*next也有*previous, 可以想像成singly linked list的高級版, 這樣便能同時知道這個node前後, 可以雙向回推, 更方便, 一般情況我們說linked list都是singly linked list
		b. stack 只需使用singly linked list, 因為stack只需操作一邊
		c. queue若不使用, 則需要用loop去找到數地的原數, loop會耗費效率, 在此使用doubly是更好的選擇(雖然會增加記憶體空間).  !!!!!   wait! I thought queue need doubly linked list. But now, I see that it doesn't! Because, first, insertion doesn't need any loop. Second, remove from front or rear, one of them needs loop. But hey, I can choose the one that doesn't need loop as the remove side of the queue! So queue doesn't need doubly linked list, either!
	E. Circular linked list
	reference:
	https://www.geeksforgeeks.org/linked-list-in-c/
    https://stackoverflow.com/questions/11867362/circular-queue-and-circular-linked-list
        a. circular linked list 可以用於建立double ended queue(他與circular array是平級概念), 且能無限制增長空間, 但同樣的無法像circular array一樣以O(1)的速度鎖定元素位置, 必須一路找下去. 記住linked nodes和array是電腦科學最基本的兩個data structure.  *我後來認為這樣的說法有點問題, 應該要說circular linked list可以用來實作queue, 而不是說他用來實作circular queue, 因為circular queue本身是指他是一個queue, 但記憶體的實作採用circular array的方式, 所以其實circular queue 就是queue, 只是強調了他用的記憶體實現方式而已
        b. Doubly circular linked list, 就好像doubly linked list可以用來實現double ended queue一樣, doubly circular linked list可以用來實現circular double eneded queue

	
|#############################################################################################################|
6. stack
	A. ADT
		a. 性質:
			I. First in Last out(FILO)

		b. 操作:
			I. push(): push an element in stack's top position
			II. pop(): return the the top element and remove it from the stack
			III. peek(): return the top element
			IIII. isempty(): return if stack is empty or not
			IIIII. size(): return how many data in stack
			ps: 基本你會發現stack push, pop 都是對top作用, 所以增刪都在同一個位置
	B. array 實作
		a. 實作要點:
			I. 一個array, 長度依需求
			II. 一個int top, 紀錄top的位置
			III. top的初始化為-1, push則+1, pop則-1

	C. linked list實作:
		a. 實作要點:
			I. 一個data
			II. 一個*next, 指向下一個node
			III. 初始node為NULL
			IIII. 與array實作的方向相反, 在header的地方push(也pop), 
			IIIII. header node代表了這個stack
			IIIIII. NULL node 代表這個stack的結束位置

|#############################################################################################################|
7. 中序式與(後序式 or 前序式)互轉的演算法
	A. infix -> postfix
		a. 需要1個stack
		b. 由左向右scan
		c. 遇到"數字"直接輸出至結果, 遇到"算符"則與stack的top比較優先級, 不能"<="top, 否則stack pop(), 直到不再"<=", 才push進stack, 注意! stack(stack只會需要存算符), 
		d. 遇到"("則push進stack, 遇到")"則pop stack, 直到遇到"("
	B. postfix的計算演算法
		a. 需要1個stack
		b. 由左向右scan
		b. 把"數字"push(pop)到stack(stack只會需要存數字)
	C. infix -> prefix
		a. 需要1個stack
		b. 由右向左scan
		c. 遇到"數字"直接輸出至結果, 遇到"算符"則與stack的top比較優先級, 不能"<"top, 否則stack pop(), 直到不再"<", 才push進stack, 注意! stack(stack只會需要存算符), 
		d. 遇到")"則push進stack, 遇到"("則pop stack, 直到遇到")"
		c. 由於是從右邊開始scan, 其結果會是reverse的, 所以你需要自己reverse過來(可以用另一個stack完成此事)
	D. prefix的計算演算法
		a. 需要1個stack
		a. 由右向左scan
		c. 把"數字"push(pop)到stack(stack只會需要存數字), 跟postfix很像, 只是計算時的次序要注意

	E. 重要練習題(裡面涵蓋一些概念, 細品):
		http://sptutor.dyu.edu.tw/DSTutor/exprDemo6.jsp


|#############################################################################################################|
8. Queue
	A. ADT
		a. 性質:
			I. First in First out(FIFO)

		b. 操作:
			I. enqueue(): push an element in queue's rear position
			II. dequeue(): return the the front element and remove it from the queue
			III. isfull(): return if queue is full or not
			IV. isempty(): return if queue is empty or not
			V. front(): return the front element of the queue
			ps: 基本你會發現queue push, pop 分別在頭和尾, 所以增刪在不同位置
	B. array 實作
		a. Linear Array實作要點(此法調整queue元素位置需要O(n), 不是很聰明):
			I. 一個array, 長度依需求
			II. 一個int front, 紀錄front的位置
			III. 一個int rear, 紀錄rear的位置
			IV. front, rear的初始化皆為-1, enqueue則rear++, dequeue則front++
			V. 當rear到最後一個位置時, 若front不是-1, 表示表示假滿, 把整個queue元素往左移動(front+1)格

		b. Circular Array(circular queue, circular buffer):
		reference: https://en.wikipedia.org/wiki/Circular_buffer
			I. 一個array, 長度依需求
			II. 一個int front, 紀錄front的位置
			III. 一個int rear, 紀錄rear的位置
			IV. front, rear的初始化皆為0, enqueue則rear=(rear+1)%n, dequeue則front=(front+1)%n. 詳細方法請參考講義或是程序實作
			V. circular method有分n-1及full-n法, n-1就是整個array有n個位置, 但只會用到n-1個位置, 此法是理論上速度最快的, 因為邏輯最為簡單, n法則是全部位置都用到, 但速度可能會較慢. 在底層世界(例如作業系統層級, linux等), 為了效率都會使用n-1法, 而"一個記憶體的位置"通常沒那麼貴, 是可以接受犧牲. 這部分是與chat-gpt討論的結果
            VI. Round-Robin(是使用circular循環概念很知名的例子), 例如假設有A, B, C三件事情要做, 則算法會A->B->C->A->B->C->A.....每次做的循環輪到某件事都會占用特定資源時間, 直到A, B, C都做完. 這其實就是CPU操作process的方式, 目的就是要使A, B, C看起來是同步執行的. 這便是Round-Robin, 其核心精神除了循環往復, 還有"每次循環的所有事件都輪流做一點點事情", 使所有事件看起來是同時推進的.

	C. linked list實作:
		a. 實作要點:
			I. 一個data
			II. 一個*next, 指向下一個node
			III. 需要一個front node和一個rear node, 兩個打包成一個queue
			III. 初始front node 和 rear node 皆為NULL
			IIII. rear node enqueue, front node dequeue
			IIIII. NULL node 代表這個queue的結束位置


|#############################################################################################################|
9. Priority Queue
	A. ADT
		a. 性質:
			I. 不一定First in First out(FIFO)
			II. enqueue任意元素(權值)
			III. dequeue最大/最小 權值的元素
            IV. 看起來, priority queue 與queue沒有任何關係, 他們insert和remove性質完全不同, 若是稱作priority stack似乎也說得過去, 我認為這個命名不是特別精準

		b. 操作:
			I. 利用heap tree(簡稱heap, 是min-heap或max-heap其中一種), 
				🤔. 定義
					😄. 為一complete binary tree(故必定平衡)
					😄. 父node必定比子node大(max heap, 反之則為min heap), 
					😄. 所有子node皆符合上述定義
			ps: heap和heap sort大有關係!!!!

			II. heap實作:
				🤔. 通常採用array的方式建構heap tree(不同於一般binary tree通常採用linked list), 這是因為heap 是complete binary tree, perfectly fit into arrays. 但不是不能採用node, 你仍需要會node的方法
                🤔. heapify有從上到下(移除最頂元素, 注意, 只能移除最頂元素!)及下到上(插入元素至最後位置)通常都是. 上到下的heapify會取最後一個元素至最頂的空缺, 這是為了使heap保持complete, 而下到上heapfiy只需往上比較parent, 直到比不贏為止.
                🤔. priority使會拉出最大值, 所以理論上不會出現移除heap首元素之外的元素, 但實際上heap的操作不局限於此, 在Dijkstra算法, 或A Search, Event Scheduling 中或有用處

            III. heap的優勢:
                🤔. 要實現priority queue 不一定非要heap, 有一個最直接的辦法, 就是enqueue的時候直接把數據照array的次序丟入, 這個enqueue只需O(1), 但是dequeue的時候, 需要一個for從array中選出最大(或最小)拿出, 然後在整組挪移, 需要O(n)
                🤔. 若是採用heap, 則enqueue要O(log(n)) 這是採用下到上的bubble up insertion.    dequeue也要O(log(n)), 採用heapify向下法, 這是樹狀結構的必然結果. heap的dequeue比基本array法有優勢.
            IV. Floyd's heap construction algorithm:
                🤔. 一個一個enqueue的時候通常採用bubble up的algorithm, 其單一耗時需O(log(n)), 建立n個元素的heap需要O(log(1) + log(2) + log(3) + log(4) + log(5)...) = O(log(n!)) = O(nlog(n). 
                🤔. Floyd's algorithm是一種一次把所有n個元素建立成heap的算法, 只需耗時O(n). 但他是一次性完成的, 一個一個就不行了. 以下是一個Floyd's algorithm之所以為O(n)的解釋, 很神奇
                https://stackoverflow.com/questions/9755721/how-can-building-a-heap-be-on-time-complexity
                    而且他其實就是一個等差-等比級數, 而等差-等比級數若n->infinity則其趨近於常數, 此稱之為Gabriel's staircase, 這使得最終耗時為O(n)
                https://en.wikipedia.org/wiki/Arithmetico-geometric_sequence
                🤔. 此法只適用把heap初始化的時候, 若需要之後再加入元素, 此法就不行了


	B. double ended queue(雙邊佇列)
		a. 性質:
			I. 兩端都可以自由插入或刪除(一般佇列是一端增, 一端刪)
			II. stack進出都同一邊. 而queue一邊進, 一邊出. Double Ended Queue (Deque) allows insertion and deletion on both sides. So, it is a combination of both Stack and Queue. 雖然命名為double ended queue, 但概念上也可叫做double ended stack. 他能模擬stack, 也能模擬queue, 且能做到兩者做不到的事.
			
		b. 實作:
			I. linked-list, 假設使用singly linked list, 由於singly linked list只支援單邊不用loop移除元素, 兩邊都要能移除元素的話, 一定有一邊要用loop, 為了效能, doubly linked list最適合實作.
			II. array, 
		c. 

	C. double ended priority queue(雙邊優先佇列)
		a. 性質:
		reference:
		https://www.geeksforgeeks.org/double-ended-priority-queue/
			I. A double ended priority queue supports operations of both max heap (a max priority queue) and min heap (a min priority queue) (簡單來說, 你能取得最大值, 也能取得最小值, 一般priority queue只能選一個)
			II. getMax() : Returns maximum element.
			III. getMin() : Returns minimum element.
			IV. deleteMax() : Deletes maximum element.
			V. deleteMin() : Deletes minimum element.
			VI. size() : Returns count of elements.
			VII. isEmpty() : Returns true if the queue is empty.

		b. 實作:
			I. 利用min-max heap:
			reference: (以下wikipedia有詳細解釋)
			https://en.wikipedia.org/wiki/Min-max_heap
				🤔. 定義:
					😄. The min-max heap property is: each node at an even level in the tree is less than all of its descendants, while each node at an odd level in the tree is greater than all of its descendants
					😄. 為一complete binary tree(故必定平衡)
					😄. The root element is the smallest element in the min-max heap
					😄. One of the two elements in the second level, which is a max (or odd) level, is the greatest element in the min-max heap 
				🤔. 操作:
					😄. insert(), 基本也是從最後一個節點插入, 然後往上比(push_up)
						😄. time complexity: O(log(n)), average and worst case
					😄. extract(), 基本也是往下比較, 兩個子節點的分支都要比較...記住min-max heap的定義, 就能發現如何處理.
						😄. time complexity: O(log(n)), average and worst case
	

	D. Symmetric Min-Max Heap(SMMH), 也可用在double ended priority queue
	reference:
	https://hackmd.io/@vRN1CwEsTLyHOsG4mC0d4Q/BkrYQ8B2V
	https://www.tutorialspoint.com/symmetric-min-max-heaps
		a. 定義:
			I. 每一個node的左子node, 都喊"我跟我的後代比起來, 我是最小的
			II. 每一個node的右子node, 都喊"我跟我的後代比起來, 我是最大的
			III. root的value為空

	E. deap (簡單版的SMMH), 也可用在double ended priority queue
	reference:
	https://medium.com/%E7%8B%97%E5%A5%B4%E5%B7%A5%E7%A8%8B%E5%B8%AB/%E5%9C%96%E8%A7%A3-double-ended-priority-queue-%E9%80%B2%E9%9A%8E%E6%A8%B9-1ae18d2ca402
		a. 定義:
			I. root為空
			II. root的左子tree, 為"min-heap"
			III, root的右子tree, 為"max-heap"
			IV. 在root左子上的任一點 i，令 j 為他在右子樹中對應的節點，i 點的權重必須小於 j 點的權重。
(p.s 若無對應點則取其父點)


|#############################################################################################################|
10. Binary Tree(Tree 也屬於 graph)
	A. 總性質:
		a. 任意tree可化為binary tree
		b. 任意node最多只有兩個degree(分支)
		c. 左右子樹有次序之分(一般tree沒有特別定義)

	B. Full binary tree
		a. 性質:
			I. 必定平衡
			II. 除了leaf以外, 所有node當成root的子樹都必定為full binary tree
			III. 最後一個level填滿
			IIII. 公式:
				🤔. N = 2^H - 1   等價於  log_2(N+1) = H, N=node數, H為高度
				🤔. lN = 2^(H-1)

	C. Complete binary tree
		a. 性質:
			I. 必定平衡
			II. 除了leaf以外, 所有node當成root的子樹也都必定為complete binary tree
			III. 最後一個level未必填滿, 但是一定由左到右填入, 不會有跳過
			IIII. 公式: log_2(N+1) = H, 無條件退位

	D. Pathological Tree
		a. 性質:
			I. H = N, H=高度, N=node數
		b. left-skewed binary tree
		c. rigth-skewed binary tree

	E. 實作
		a. 使用array
			I. 優點:
				🤔. 對於full B.T完全沒有浪費空間
				🤔. 容易取得某node之左右子node, 以及父node的資料
					😄. 某node的編號為i則:
						😄. 左子node編號為2i, if 2i>N, 左子node不存在
						😄. 右子node編號為2i+1, if 2i+1>N, 右子node不存在
						😄. 父node編號為[i/2]([]:無條件捨位), if [i/2]<1, 則父node不存在
			II. 缺點:
				🤔. 節點增刪不易, 若空間不構, 要重新宣告array
				🤔. skewed B.T極度浪費空間

		b. 使用linked list(大部分實作還是用這個)
			I. 優點: 
				🤔. node增刪容易
				🤔. 對於skewed B.T
			II. 缺點: 
				🤔. 不易取得父node
					😄. 因為每個node只存左子node以及右子node
				🤔. link空間仍浪費約一半
					😄. 因為所有leaf的link皆沒有用到
			III. 實作:
				🤔. 每個node皆需要
					😄. data
					😄. left child node(c語言可用指標)
					😄. right child node(c語言可用指標)
				🤔. root代表了這棵樹
		c. Tree Serialization(包含a提到的array法, 其實就是把complete tree灌入array中, 空的地方補#或null, 此法又叫做level-order), 還有常見的empty symbol preorder(其實還有postorder) Method. 其實level-order是breadth-first廣度優先循序的一個典型例子, 而preorder, inorder, postorder則都是depth-first深度優先的例子.
				🤔. 由於tree是二元以上的, 而serialiation本身是把二元的數據注入一元的資料格式中, 這種做法與數學上的高維用低微表示有關, 可以研究一下
			
	 
|#############################################################################################################|
11. Binary Tree Traversal(操作上幾乎都使用recursive)
	A. breadth first(廣度優先) Traversal(其實就是level-order traversal)  -----> sibling 先走, 通常需要使用queue(explicitly顯然使用, 因為recursive無法simulate queueu, in short recursion can't naturally simulates BFS). 需要使用queue的原因是因為breadth-first本身其實simulate "first in first out", 其實就是queue的概念
		a. 由左到右(其實也可以設定成由右到左, 這叫做reverse level-order traversal or zigzag traversal, 只是比較少用)
		b. 再由上到下
		c. 簡單來說就是同一level的先處理完再處理下一個level
        d. breadth-first與queue(first in first out)的關聯, 以下是一個很好的範例:
            I. (root) 
                A
               / \
              B   C
             / \
            D   E
            II. 以上範例利用一個queue, 訪問過程如下
                🤔. Start:  [A]
                🤔. Visit A → enqueue B, C →  [B, C]
                🤔. Visit B → enqueue D, E →  [C, D, E]
                🤔. Visit C → (no children) → [D, E]
                🤔. Visit D → (no children) → [E]
                🤔. Visit E → (no children) → []

	B. Depth first(深度優先) Traversal  ----> 子node先走, 這本身就包含recursive的概念, 因為把子node當作root進行重複操作, 通常需要使用stack, 以下的order通常implicitly(隱含)的使用stack的概念, 而非直接, 這是因為recursion(遞迴)的操作本身就simulate stack了(recursion naturally simulates DFS), 因為depth-first simulate "first in last out", 其實就是stack的概念.
		a. 前序(preorder, DLR) Traversal, preorderTrav()
			I. 採用recursive
			II. time complexity: O(n)
            III. 一個問題:
                🤔. In preorder, if it's depth-first, but i first deal with the level-1's first element, and then level-2's first element, and then level-3's first element, it's still a depth-first. However, it seems like it's not "first in last out" any more, because it deal with the element it encounter first. In this situation stack still fit?
                    😂. the answer is Yes. Here's why: Even if you process (do some work on) a node immediately when you encounter it, DFS is still controlled by the “path-first” structure, meaning:

                        😂. You go down one branch as far as possible before moving sideways to other branches.

                        😂. To remember where to return when you finish a branch, you still need FILO memory (stack), whether explicit or via recursion.

                    😂. Your example — level 1’s first → level 2’s first → level 3’s first — is just preorder DFS. You’re doing “visit now, then go deeper,” but the backtracking still requires a stack to hold the unfinished siblings. The only time DFS wouldn’t be “FILO” is if you somehow forbid backtracking entirely (which wouldn’t be DFS anymore — you’d just be walking one chain).
                    😂. Here's a more precise explanation, suppose we have preorder function code like below:
                    ```c
                        preorder(root) {
                            show();
                            preorder(root->left_child);
                            preorder(root->right_child);
                        }
                    ```
                        😂. show() runs right away — nothing is on the call stack for it when it finishes, because it’s just a simple function call in the current frame. 
                        😂. preorder(root->left_child) is called.
                            😂. This is a new function call → pushes a new frame onto the call stack.
                            😂. It will finish completely before you ever start preorder(root->right_child). 
                        😂. preorder(root->right_child) is not executed yet.
                            😂. The fact that this line of code is still pending means the current function frame remains on the stack until root->right_child traversal is done.

		b. 中序(inorder, LDR) Traversal, inorderTrav()
			I. 採用recursive
			II. time complexity: O(n)

		c. 後序(postorder, LRD) Traversal, postorderTrav()
			I. 採用recursive
			II. time complexity: O(n)

		d. 給(inorder和preorder)排序結果, 回推binary tree
			I. preorder的root必定在最前面
			II. inorder的root必定在左右子tree的inorder中間
			III. 重複I, II

		c. 給(inorder和postorder)排序結果, 回推binary tree
			I. postorder的root必定在最後面
			II. inorder的root必定在左右子tree的inorder中間
			III. 重複I, II

		d. 給(preorder和postorder)排序結果, 回推binary tree
			I. 可能有很多種結果, 故無法

		e. **空標記法**: 此法和以上的普通標記規則不大相同, 假設ABC為前序排序, 空標記法的規則明確說明B必定是A的左子節點, C必定為B的左子節點, 這點與普通標記法不同. 普通標記法之下ABC排序, B不一定是A的左子節點.  另外, 在空標記的規則之下, 可以(且必定)唯一決定樹的樣子, 例如ABC是可以唯一決定的, 例如更複雜的ABC##D#E(其中#為空節點), 直接按照規則嘗試建構樹則可知道.

		f. 計算B.T node 總數, count()
			I. 採用recursive

		g. 計算B.T 高度, height()
			I. 採用recursive

		h. B.T每一node左右子tree交換, swap()
			I. 採用recursive

    C. Traversal的用處, 前面大致提到traversal的用處, 兩種traverasal(DFS, BFS), 這邊總結, 其實traveral的目的主要有以下
        a. Serialization(turning the tree into a sequence/string)
            I. preorder Serialization (DFS)
            II. inorder Serialization (DFS)
            III. postorder Serialization (DFS)
            IV. level-order Serialization (BFS)
        b. Searching
            I. finding a value 
        c. Calcualting
            I. sum
                🤔. DFS(preorder, inorder, or postorder)    
            II. height
                🤔. DFS(postorder)    
            III. min/max/etc
                🤔. DFS(any order)
        d. Modifying
            I. updating the value or deleting nodes
        e. Building output
            I. printing the node sin human-readable order
        

		
|#############################################################################################################|
12. Binary search tree
	A. 定義:
		a. 左子樹所有Node鍵値均小於Root鍵値
		b. 右子樹所有Node鍵値均大於Root鍵値
		c. 左、右子樹亦是Binary Search Tree

	B. 建立(Insertion):
		a. 準備一段資料, 例如array
		b. 將array的首個資料當作root
		c. 接著按照binary search tree定義依序填入
		d. 平均時間複雜度: O(nlog n)
			I. 推導:
				🤔. total time complexity = log(1) + log(2) + log(3) + ... log(n) = log(n!)
				🤔. log(n!) = nlog(n) when n->∞, according to stirling formula(use intergral)
			若是不信, 以下還有一個討論:
			https://stackoverflow.com/questions/8118221/what-is-ologn-on-and-stirlings-approximation

		e. 最差時間複雜度: O(n^2)
			I. 剛我是一skewed tree
 
	C. 查找(search)
		a. 最壞情況O(n), 這棵樹剛好是skewed tree
		b. 平均情況O(log n)

	D. 插入(insert)
		a. 最壞情況O(n)
		b. 平均情況O(log n)
		c. 方法: 沒什麼好講的, 照BST定義

	E. 刪除(delete)
		a. 最壞情況O(n)
		b. 平均情況O(log n)
		c. 方法:
			I. 删除的节点是叶节点（没有子节点）：
				🤔. 直接删除该节点，不需要进行额外的调整。
			II. 删除的节点有一个子节点：
				🤔. 用该节点的子节点替代它。
			III. 删除的节点有两个子节点：
				🤔. 找到该节点的前驱或后继节点，用前驱或后继节点的值替换该节点的值，然后删除前驱或后继节点。前驱节点是左子树中的最大节点，后继节点是右子树中的最小节点。

	F. 排序(sort), BST也可以用於排序(但資料必須先建立成BST)
		a. 用inorder traversal, 即可得到"由小到大"的排序結果
		b. 時間複雜度自然是traversal的複雜度O(n)
		c. 在tree sort實作中, 需要把"create tree"的複雜度也算進來, 所以總共還是要O(nlog(n))

	
|#############################################################################################################|
13. Tree 或 forest 化 Binary tree
	A. Tree 化 binary tree
		a. sibling之間建立link
		b. 每一個node只保留最左子node的link和剛才建立的sibling link, 其餘link皆斷

	B. Forest(many unlinked tree) 化 binary tree
		a. 所以tree皆化binary tree
		b. 將binary trees們的root建立link
		c. 以第一顆tree的root為新root

	C. 其實Tree(多元)化binary tree本質與Forest化Binary tree是在做一樣的事情, 因為Tree化binary tree的過程, 實際上就是把每個節點的子樹集合化成一個binary tree, 然後遞迴到root去. 多做幾個範例就可以體會到其精神. 其實很多實際結構都類似Tree(多元), 這些多元tree其實後來都用二元tree的方式去處理了. 所以數學上tree化binary tree是很重要的


|#############################################################################################################|
14. 圖(Graph), 利用集合(set)的概念會很好理解
	A. 無向圖(undirected graph)
		a. G=(V, E), V=vertex, E=edge
		b. E通常表示(vi, vj), 其中vi, vj為兩相鄰vertex
		c. (vi, vj) == (vj, vi)

	B. 有向圖(directed graph)
		a. G=(V, E), V=vertex, E=edge
		b. E通常表示(vi, vj), 其中vi, vj為兩相鄰vertex
		c. (vi, vj) != (vj, vi)

	C. 完整圖(complete graph)
		a. 無向圖的完整圖
			I. 所有vertex都與其他vertex形成一個edge
			II. 若有n個vertex, 則共有n(n-1)/2個edge
		b. 有向圖的完整圖	
			I. 所有vertex都與其他vertex形成兩個(來回)有向邊edge
			II. 如有n個vertex, 則共有n(n-1)個edge

	D. 子圖(subgraph)	
		a. 集合V(G') 屬於 集合V(G) 且 集合E(G') 屬於 集合E(G), 則G'為G的subgraph

	E. 路徑(path)
		a. 定義:
			I. vertex v到v', 經過的所有edge的集合
		b. 長度(path length):
			I. edge的個數
		c. 簡單路徑(simple path):
			I. 除了頂點和終點, 其他點皆不同
				🤔. 若頂點和終點相同稱為"迴圈(cycle)"

	F. 連通(connected)
	reference: (以下參考是關於"連通"以及"連通分量(connected component)"的一些重要概念
	https://web.ntnu.edu.tw/~algo/ConnectedComponent.html
	https://web.ntnu.edu.tw/~algo/ConnectedGraph.html
		a. for 無向圖
			I. 任意vertex與任意vertex之間皆有path存在, 即表示此圖connected
			II. 若不連通, 則必定可以分成不相連子圖

		b. for 有向圖
			I. 強連通(strongly connected)
				🤔. 任意vertex與任意vertex之間皆有path(雙向)存在
			II. 弱連通(weakly connected)
				🤔. 任意vertex與任意vertex之間皆有path(單項或雙向)存在
				🤔. 但是至少有一對vertex之間只有單向path(只能往, 不能返)
			III. 不相連(disjoint)
				🤔. 並非任意vertex之間皆有path(單向或雙向)
				🤔. 圖可以分成不相連的子圖

		c. 連通元件(connected components):
			I. 子圖中的最大邊界連通圖(可能有好幾個), 通常一個無向圖如果好幾個部分不相連, 則這些不相連的子圖, 都是connected components

		d. 圖的"關節(articulation point)"
			I. 定義:
				🤔. A joint in a graph is a vertex whose removal increases the number of connected components.
		
		e. 圖的"橋"
			I. 定義:
				🤔. 與關節類似, 只是他是"邊"

		c. 雙連通(biconnected):
		reference:
		https://web.ntnu.edu.tw/~algo/ConnectedComponent.html
			I. 定義:
				🤔. for 無向圖: 任意一個節點被移除, 圖仍然是連通的, 稱此圖為"雙連通"
				🤔. for 有向圖: 一個雙連通的有向圖中，對於任何兩個頂點v和w，都有兩條從v到w的有向路徑，且除了v和w以外沒有其他公共頂點。

	
	G. 分支度(degree):
		a. for 無向圖
			I. 就是v有幾個e

		b. for 有向圖
			I. 入分支度(Indegree)
				🤔. 指向v的e個數
			II. 出分支度(Outdegree)
				🤔. 指出v的e個數
			III. 公式
				🤔. e = total_sum(indegree) = total_sum(outdegree)  (某點的入, 必定是某點的出!, 有入必有出)
				🤔. 從以上公式可以知道, 想要算總e, 只要算總indegree 或是 總 outdegree即可

	H. 資料結構(數學)如何表示graph?
		a. Adjacency Matrix(相鄰矩陣)
			I. for 無向圖(undirected graph)
				🤔. 此matrix為n*n, 縱向表n個nodes, 橫向也表n個nodes, 有link就1, 沒link就0
				🤔. 必為對稱矩陣
				🤔. (vi, vj)是否存在:
					😄. 檢查A[i][j]是否為1
						😄. 時間複雜度O(1)
			
				🤔. 求vi的e數:
					😄. 第i列 or 第i行 元素總和
						😄. 時間複雜度O(n) ---need 1 loop

				🤔. 求總e數:
					😄. total_sum(A[i][j])/2
						😄. 時間複雜度O(n^2)

			II. for 有向圖(directed graph)
				🤔. 矩陣的形式與無向圖一樣
				🤔. (vi, vj) <> (vj, vi), 故矩陣不一定對稱
				🤔. (vi, vj)是否存在:
					😄. 同無向圖
				
				🤔. 求vi的e數
					😄. Out-degree：求第 i 列之元素値總和。
						😄. 時間複雜度O(n) ---need 1 loop
					😄. In-degree：求第 i 行之元素値總和
						😄. 時間複雜度O(n) ---need 1 loop

				🤔. 求總e數:
					😄. total_sum(A[i][j]
						😄. 時間複雜度O(n^2)
		
		b. Adjacency List(相鄰串列)
			I. for 無向圖:
				🤔. 特性:
					😄. 跟linked list長的有點像, 但是意義完全不一樣, 不可搞混
					😄. 有n個node的graph, 用1維矩陣表示為A[i]
					😄. A[i] = vj|link -> vk|link -> vl|link........
					😄. A[i]存放的就是一個Adjacency List, 他表示與vi相連的vertex
					😄. Adjacency List的順序其實沒差啦 vj|link -> vk|link or vk|link -> vj|link都可
					😄. 此法所需node數(v|link)為2e (因為vi->vj, vj->vi都記錄下來了)
				
				🤔. (vi, vj)是否存在:
					😄. 檢查A[i]的adjacency list是否有node vj
						😄. 時間複雜度O(e/n) --- 這是平均和最差時間複雜度, 
							😄. 因為A[i]的adjacency list 長度正比於e
							😄. 平均而言len(A[i]) = e/n  (平均把所有node分散到每個A[i])
							😄. 你在長度e/n的串列做線性search, 自然複雜度O(e/n)
				🤔. 求vi的分支度(degree):
					😄. 求A[i]的adjacency list長度	
						😄. 時間複雜度O(e) ---
							😄. 因為要一個一個數, 所以跟複雜度search一樣

				🤔. 求graph的e數:
					😄. node總數/2
						😄. 時間複雜度O(n+e)
							😄. 找到A[i]需要O(1), traversal A[i]的adjacency list需要O(e), 所有處理一個A[i]總共要O(1+deg(vi))
							😄. 所有A[i]加總, O(1+deg(v1) + 1+deg(v2) + 1+deg(v3)... = O(|V| + |E|)
						😄. 當e為最大的時候e = n(n-1)/2, so 時間複雜度O(n+e) = O(n^2)
			
			II. for 有向圖(directed graph)
				🤔. 與無向圖幾乎一模一樣
	


|#############################################################################################################|
15. 圖(Graph)的traversal 
	A. 需設定每一個node的visited flag
		a. flag=0: 尚未拜訪
		b. flag=1; 拜訪中
		c. flag=2; 已拜訪

	B. 深度優先(depth first traversal)----> 子node 先走
		a. 過程
			I. 選擇一個起始vertex vi, 
			II. 選擇一個與vi相連接且flag=0的vj, 並設定vj為新的起始點
			III. 以vj作為新的起始, 重複上述步驟
			IIII. 直到找不到下一個flag=0的vertex 

		b. 實作方法, 由於DFT有"recursive"的概念在其中(子node當作新root), 故可用stack來保存走訪中的vertex
			I. 選擇一個起始vertex vi, push到stack中
			II. 若stack不為空
				🤔. 從stack中pop()出top, 視為已拜訪
				🤔. top相接的所有flag=0的vertex都push進stack  ---> 重複執行
				🤔. 若所有vertex都被拜訪過, 而stack仍不為空, 則stack清空
			III. 若stack為空, 則traversal 結束
			IIII. DFT的順序不unique, 除非你自己設定按照某種排列


	C. 廣度優先(breadth first traversal) ----> sibling node 先走(遍歷所有silbling)
		a. 過程:
			I. 選擇一個起始vertex vi,
			II. 選擇一個與vi相連接且flag=0的vj, 
			III. 重複上述步驟(注意! 這個時候起始點還是vi, 沒有變)
			IIII. 接著對剛才走訪過的sibling重複上述步驟
			IIIII. 直到找不到下一個flag=0的vertex

		b. 實作方法, 使用"queue"來保存走訪中的vertex, 同樣是recursive
			I. 選擇一個起始vertex vi, enqueue到queue中
			II. 若queue不為空
				🤔. 從queue中dequeue()出front, 視為已拜訪
				🤔. front相接的所有flag=0的vertex都enqueue進queue---> 重複執行
			III. 若queue為空, 則traversal 結束
			IIII. BFT的順序不unique, 除非你自己設定按照某種排列

	D. 用處:
		a. 判斷graph是否連通
		b. 可以找出連通的單元(組件, 或者說區塊吧)
		c. 起概念是"拓撲排序", "最短路徑"的基礎


|#############################################################################################################|
16. AOV(Activity On Vertex)  ---> 其實就是一個"有向圖"的應用
	A. 定義:	
		a. 其實就只是一個有向圖(directed graph), 不用一定要連通(兩個獨立的不互聯的節點也可以拓撲排序)
		b. vertex表示"工作(activity)"
		c. edge表示"工作的次數"(有方向嘛~)

	B. 拓撲順序(topological order, 或叫"拓撲排序") 
		a. 定義:
			I. 首先你的AOV圖不能有cycle
			II. 選一個起始vertex v_s和終點vertex v_e
			III. path v_s到v_e之間經歷的所有vertex(就是traversal!), 不能有vi->vj但是vj的排序在vi之前

		b. 卡恩演算法(精神其實就是, 每次都找沒有父node的節點, 因為這表示該節點的所有先導任務都完成了):
			I. 找出一個indegree=0的vertex
			II. 將此vertex輸出至result, 且刪除他的所有outdegree edges
			III. 重複I~II, 直到所有vertex接輸出至result, 或剩下的vertex都有indegree
			IIII. if 不是所有vertex都輸出至result, 則 沒有topological order
				🤔. 注意! AOV network圖若有cycle, 則必定沒有topological order(也就是無法決定那個工作先做)
				🤔. 注意! 不具cycle的AOV network圖, 則topological order >= 1組(不一定唯一)

		c. 經典現實例子: 
			I. 某校的選課系統中，存在這樣的規則：每門課可能有若干門先修課，如果要修讀某一門課，則必須要先修讀此課程所要求的先修課後才能修讀

	C. 資料結構如何表示?
		a. 使用adjacency list(不用adjacency matrix因為在這個case裡有點麻煩)
			I. 把A[i]改成一個2*n的矩陣A[c][i],  (就是多出一條來紀錄每個vertex的indegree數量)
				🤔. A[0][i] = count (vi的indegree數量)
				🤔. A[1][i] = vi的adjacency list

			II. 如何實作刪除vi的outdegree edge? (假設我們把vi丟到result, 同時也得刪掉她的outdegree edge才行)
				🤔. 假設A[1][i] = v1|link -> v2|link -> v3|link, 則
				🤔. A[0][1]--, A[0][2]--, A[0][3]--

				
				
|#############################################################################################################|
17. AOE(Activity On Edge, 就是常見的加權路徑圖)  ---> 也是就是一個有向圖的應用, 只是edge多了"加權值", 即加權圖
	A. 定義:	
		a. 其實就只是一個有向圖+edge權重
		b. vertex表示"事件(event)"
		c. edge表示"工作(activity"
			I. 有加權值, 通常表示所花費的time

	
	B. 應用:
		a. 求完成計畫藍圖所需的最少時間(最起碼), 即是求關鍵路徑(critical path, 必須建立在拓撲排序之上, 否則狗屁不通)
			I. 求start_event->end_event之最長路徑(就是Critical path, 臨界路徑)
				🤔. 為何是求最常, 這裡有點反常識, 一下有個說明
					https://zhuanlan.zhihu.com/p/340950042

					😄. 其實就是所有的路徑其實要同時執行, 所以最長時間的路徑才是影響結果的關鍵
	
		b. 如何縮短完成計劃藍圖的時間?
			I. 找到所有critical path的交集工作(即egde), 並盡可能縮短他的權重

	E. 數據結構:
		a. Adjacency Matrix:
			I. 與一般Adjacency Matrix差不多, 只是本來有邊是以1表示有無, 現在變成用edge的長度(權重)
		b. linked list
			II. 與一般Adjacency List 差不多, 只是linked list節點多存了"邊長(權重)"
				

		
|#############################################################################################################|
18. 延伸二元樹(Extended binary tree)  ---> 用在霍夫曼樹(Huffman's tree)
	A. 定義
		a. 大部分的時候我們會使用linked list來表示B.T, 但是, 這樣leaf的link就沒有使用到, 挺浪費!

		b. Extended B.T會在leaf link連上一些特定的東西, 來幫助運算, 也順便空間在利用
			I. leaf link連上的node稱之為 "External Node" 或是 "Failure Node(因為如果搜索資料到了此處, 表示根本沒有要找的資料, 故return fail)"
			II. 其餘原本的node稱值為 "Internal Node"
			III. 根據B.T的性質, 很容易可以得到num(External_Node) = num(Internal_Node)+1

	B. 一些常用概念:
		a. 若I=sum_i(path_len(root->vi)), vi為internal node
		b. 若E=sum_i(path_len(root->vx)), vx為external node
		c. 則E=I+2n,  其中n為internal node總數
		d. 又若n固定, 則E與I成線性關係
		e. 越是balance的tree, E與I就愈小
		f. 但若是external node有加權值得時候, 上一概念 不一定成立!!!!(huffman's tree不一定要平衡!!)

	C. Weighted External Path length(W.E.P.L, 加權外部路徑長度), 其實就是每一個External path都要乘上各自的weight
		a. 定義:
			I. 即是 W.E.P.L = sum_x(path_len(root->vx)*weight_x)

		b. 性質:
			I. 沒有加權的情況下, 越平衡的樹, 搜索效率會越高, 總路徑長度(同時...平均路徑長度)會越短
			II. 但是有加權過後, 就不一定了

		
	D. 給定n個weight(也就是一定有n個external node), 如何求Min. W.E.P.L,  以及他對應的tree(就是huffman's tree了)
		a. Huffman's Algorithm: (與shannon theory裡面的entropy 大有關係, 但先不討論...)
			I. 設W為給定的weights的集合
			II. 算法如下:
				🤔. 將W中所有weight都設定為external node
				🤔. 自W中取出兩個最小的權重並相加(wi+wj), i
				🤔. 將(wi+wj)設定為wi和wj的父internal node
				🤔. 將(wi+wj)放入W中
				🤔. 重複上述步驟, 直到W中只剩一個weight(即是這顆huffman tree的root)

			III. Huffman Algo的精神:
				🤔. weight越大越應該離root近
				🤔. 從最小的weight開始建tree, 可以使大的weight更靠近root

			IV. 時間複雜度:
				🤔. building huffman tree: O(Nlog(N))
				🤔. Encoding O(N)
				🤔. Decoding O(N)

			V. 經典應用:  編碼/解碼 的最佳情況
				🤔. 要傳遞一段英文訊息, 共有26個英文字母代號可用, 每個字母出現的頻率已知, 若要使用2進制(0, 1)來為26個字母編碼, 怎樣會節省空間, 最有效率?
					😄. 這個問題中, 字母頻率其實就是weight
					😄. Huffman tree root往左走代表0, 往右走代表1
					😄. 如此的編碼將會在平均上使傳遞的訊息 用最少的bit來表示, 最節省空間(因為他用了"min W.E.P.L", 同時編碼/解碼正比於bit數 所以 在平均上 也會達到最快(不可能更快了, 這跟信息entropy理論有關)
					😄. 其精神便是越常出現的字母, 其編碼使用的bit數要越少(即path長度越短)

			VI. 程式實作: 
			reference:
			以下包含了python, c, java, c++(除了python以外都很麻煩)
			https://www.programiz.com/dsa/huffman-coding

				🤔. 由於需要"選取最小的權重出來", 你會需要"priority queue", 所以用通常會使用到min-heap來實作
				🤔. 設計步驟: (較難理解, 最好搭配code一起看)
					😄. 先用array的辦法build出一顆min heap tree, 此heap的每一個node都存"一個huffman tree node"
					😄. 這裡比較繞...此huffman tree是一個linked list, 所以她的root代表了這棵樹, 
					😄. 而huffman tree的root就是按照huffman algorithm, 把heap全部的node都extract之後最後一個node
				

|#############################################################################################################|
18. 自平衡二元搜索樹(self-balancing binary search tree)----> binary search tree的理論延伸 
	reference:
	https://zh.wikipedia.org/zh-tw/%E5%B9%B3%E8%A1%A1%E6%A0%91

	A. 目標:
		a. 我們知道binary search tree越是balance, 平均search效率就越高: log(n)--->事實上是最高了...
		b. 但我們在建構binary search tree的時候, 不見得會得到一顆平衡的樹, 我們需要些辦法獲得平衡的BST
		
	B. 常見的算法(方法), 大部分都使用了"旋轉(rotate)"的操作:  每次insert都需要log(n)複雜度+1個rotate: O(log(n)
		a. AVL tree(AVL是人名縮寫) 
			I. 定義:
				🤔. 為一顆binary search tree, 且height 平衡(balance)
				🤔. 嚴格定義平衡(balance):
					I. |height(Lchild(root))-height(Rchild(root))| <= 1,  
						😄: height(Lchild(root)) 為 左子tree高度, 
                                                😄. height(Rchild(root)) 為 右子tree高度
				🤔. 左右子tree也是AVL tree

			II. 平衡因子(Balance factor, BF):
				🤔. BF = h_l-h_r
				🤔. 在AVL tree中, BF只可能是: -1, 0, +1

			III. 建構時出現的不平衡情況:
				🤔. LL不平衡: 
					😄. height(Lchild(root))-height(Rchild(root)) > 1, 
					😄. 且height(Lchild(Lchild(root))) > height(Lchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) > 1
					😄. 且height(左子tree的左子tree) > height(左子tree的右子tree)
				🤔. LR不平衡
					😄. height(Lchild(root))-height(Rchild(root)) > 1, 
					😄. 且height(Lchild(Lchild(root))) < height(Lchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) > 1
					😄. 且height(左子tree的左子tree) < height(左子tree的右子tree)
				🤔. RL不平衡
					😄. height(Lchild(root))-height(Rchild(root)) < -1, 
					😄. 且height(Rchild(Lchild(root))) > height(Rchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) < -1
					😄. 且height(右子tree的左子tree) > height(左子tree的右子tree)
				🤔. RR不平衡
					😄. height(Lchild(root))-height(Rchild(root)) < -1, 
				😄. 且height(Rchild(Lchild(root))) < height(Rchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) < -1
					😄. 且height(右子tree的左子tree) < height(左子tree的右子tree)

			IIII. 調整不平衡的算法, 採用旋轉的方法:  ----> 快速公式(適用AVL tree)
				🤔. 其實你根本不用管是哪種不平衡, 只要記住以下
					😄. 最大的值擺在R_child, 最小的值擺在L_child, 中間值擺在root
					😄. 若發生不平衡的插入值不再以上3個值之內, 則此插入值重插
				🤔. 調整的時候會平衡所有子node, 不會對父node造成任何影響, 所以理論上, 你只需要選轉一次即可(重要概念!!考過!)

			V. 旋轉詳細說明:   ----> AVL解題時用之前的快速公式即可, 但以下的概念還是十分有用, 要會
			reference: 以下有個比較清晰的參考說明:
			https://simpletechtalks.com/avl-tree-self-balancing-rotations-right-left-rotation-explained/
				🤔. 若發生RL不平衡, 則在先從較低level的開始調整, L不平衡的部分的連線元素, 進行順時針旋轉(稱之為右旋right rotation), 然後對剩餘L不平衡的部分(較高level)的連線元素, 進行逆時針旋轉(稱之為左旋left rotation).  發生LR不平衡, 則倒過來做. 若發生RR不平衡則只需對高level元素做left rotation.  LL不平衡則只需對高level做right rotation

			VI. remove node:
			reference:  --->以下有很清楚的說明, 就是移植不斷往下平衡
			https://www.cs.emory.edu/~cheung/Courses/253/Syllabus/Trees/AVL-delete.html
				🤔. 從AVL樹中刪除，可以通過把要刪除的節點向下旋轉成一個葉子節點，接著直接移除這個葉子節點來完成。因為在旋轉成葉子節點期間最多有log n個節點被旋轉，而每次AVL旋轉耗費固定的時間，所以刪除處理在整體上耗費O(log n) 時間。

			VII. 高度-最少節點數定理
				🤔. 建立高度h的AVL tree, 最少需要多少節點?
					😄. 答: S(h) = S(h-1) + S(h-2) + 1, 原理如下:
						😄. 首先, AVL tree的高度為h, 則任意node的左右子樹應為一個高度為h-1, 另一個高度為h-2, 這是最少節點的時候必定符合的
						😄. 所以分別計算左右子樹的最少節點, 然後相加: S(h-1) + S(h-2), 最後在加上本身自己也算一個節點, 所以S(h)為 S(h-1) + S(h-2) + 1, 遞迴~
						😄. 只要知道S(1)=1和S(2)=2就能計算所有的S(n)了

					😄. 答2: S(h) = F(h+2) - 1,   F(n)為費氏數列第n項
						😄. 證明較難, 先跳過

				🤔. 範例1:
					😄. 高度3的AVL tree, 至少要Fibonacci(3+2)-1 = Fibonacci(5)-1 = 4個節點
					😄. 高度3的AVL tree, 最多要?
						😄. 就是盡量建立成full tree, 所以答案是 2^h - 1 = 7

				🤔. 範例2:
					😄. 一個AVL tree有15個nodes, 則他的最大高度為?
						😄. Fibonacci(h+2)-1 <= 15 < Fibonacci(h+2+1)-1
							😄. 答: h = 5
					😄. 一個AVL tree有15個nodes, 則他的最小高度為?
						😄. 就是儘量建立成full tree, 答案: log(n+1)=4 (無條件退位, 跟complete tree的高度算法是一樣的)
		
		b. 樹堆(Treap)

		c. 伸展樹(Splay tree)
	
		d. 紅黑樹(Red-black tree)
		reference:
		https://tigercosmos.xyz/post/2019/11/algorithm/red-black-tree/
		https://www.youtube.com/watch?v=UaLIHuR1t8Q
			I. 定義:	
				🤔. root必為黑
				🤔. root到葉節點的任意路徑不能出現連續兩個紅
				🤔. 從root到任意只有一個children的節點, 其路徑上的黑色節點數量必定彼此相同
			II. Insertion
				🤔. 若為空, 則插入黑色節點為root
				🤔. 不為空, 一律插入紅色節點
					😄. 若該紅色節點的parent為黑, 則ok
					😄. 若該紅色節點的parent為紅, 違反紅黑樹定義, 需要調整
						😄. 若該節點的parent的sibling為紅
							😄. 首先我們為了讓紅不要連續, 我們把parent紅轉黑, 
							😄. 因為此時parent的sibling也為紅, 則為了使定義第三點成立, parent的sibling也要轉黑, 黑色數量才會對
							😄. 但是此時parent和parent silbing兩個分支都多了一個黑色節點, 這表示與其他非這兩分支的分支比較起來, 多了黑色數量, 這與第三點定義又矛盾了, 為了修復, 你必需在把這兩個parent的parent顏色也改變, 才能確保符合定三點定義(如果改變過程又出現RR連續, 你重複相同操作步驟一路刷到root)
						😄. 若該節點的parent的sibling為黑或為空, 則該節點rotate且轉黑
							😄. 首先, 因為我們一樣為了讓紅色不要連續, 把parent紅轉黑, 
								😄. 此時若parent的sibling為空, 則表示你無法藉由改變parent的sibling的顏色來使結構滿足定義三, 此時唯一的辦法就是使用rotate, 一樣大擺右, 小擺左, 中間擺中間, 接下來依照定義自由發揮了吧. (或利用AVL tree的概念RL不平衡就用R旋轉+L旋轉)
								😄. 此時若parent的sibling為黑, 則表示你依舊無法藉由改變parent的sibling的顏色來使結構滿足定義三, 因為parent的sibling本來就是黑色, 你改成紅色反而讓黑色數量減少了, 而parent改成黑色後自己的分支黑色數量增多, 所以違反定義三.   此時唯一的辦法就是使用rotate, 一樣大擺右, 小擺左, 中間擺中間, 接下來依照定義自由發揮了吧. 
		e. 加權平衡樹(weighted balanced tree)


						
|#############################################################################################################|
19. m-way search tree (binary search tree的擴展)   *****---- degree>2的樹, 高考好像沒考過*****
	A. 定義:
		a. 每個node可以有多個key, 但key的數量不超過(m-1)個, 但也至少要有1個key
		b. 每個key各有兩個degree(左右)
		c. 根據以上, 可以得到一個結論: 每個node的degree有m個, 且至少2個(你至少有1個key)
		d. node的key由小排列到大
		e. 各node的子tree同樣是m-way search tree
		f. 子tree內的所有資料皆小於他的父key
		
	B. 目的:
		a. 資料量龐大的時候, binary search tree高度太大(搜索慢), 如果node的degree多一點, 可以有效降低高度
			I. 外部搜索(External Search)
				🤔. 把資料放在記憶體以外的儲存空間進行處理
			II. 內部搜索(Internal Search)
				🤔. 把資料放在記憶體處理, 但是空間很有限, 資料量不能太多


|#############################################################################################################|
20. B-tree           
	A. 定義:
		a. 即是一個balanced的m-way search tree! 
		b. 除了root及failure node之外, 其餘node的degree介於[m/2]至m  ----> 重點!!!
		c. 所有failure node在同一個level, 所有leaf在同一個level

	B. 最簡單的B-tree:    2-3 tree:
		a. 就是m=3(你如果m=2就變成balanced B.S.T), 根據定義任意node的degree介於2到3

	C. 操作
	reference:  below is a very good example talks about how to insert and delete
	https://www.tutorialspoint.com/data_structures_algorithms/b_trees.htm
		a. Insert:
			I. 過程可能會遇到node裡面key數量超過m-1的狀況(overflow), 需要"split"
				🤔. split的辦法:
					😄. 將第[m/2]key 拉到父node, 其餘keys拆分成左右2個node
					😄. 這個辦法能保證你的B-tree一直是B-tree

		b. delete:
			I. 先找到要delete的鍵值, 假設為x
			II. 過程可能遇到node裡面key數量低於[m/2]-1(underflow), 這個時候需要"rotate"或"combination", 以下分為2種狀況:
				🤔. x在leaf:
					😄. 檢查拿掉x後有無underflow
						😄. 若無則OK
						😄. 如有
							😄. 嘗試rotation
							😄. rotation 不可行, 則做combination
							😄. 接著檢查combination完後, 父node有無underflow
							😄. rotation, combination的發法請參考講義(有點複雜)
							😄. rotation, combination的大原則如下:
								😄. rotation: 若x node underflow, 則其sibling送key給父node, 父node在送key給x node
								😄. combination: 若x node underflow, 且其sibling沒有key可送, 則父node送key給x node, 並把x node與其他sibling合併

				🤔. x在非leaf node:
					😄.  以x的右子樹中之最小鍵値取代x；或
					😄.  以x的左子樹中之最大鍵値取代x
					😄. 左右子樹都沒人可以借, 則x的parent和他所有的chilren combine.

	D. 應用
		a. 資料庫索引(index)
			I. 其採用的辦法正是B-Tree資料結構, 
				原理: 為何可以增加搜索數據的速度, 其實不難理解, 資料存入資料庫若是沒有任何資料結構, 那搜索將採用線性搜索, 也就是時間複雜度是O(n), 如果做了B-Tree的索引結構, 那搜索速率自然提升到O(log(n)) 補充: 雖然索引(index)會使搜索效率提升, 但是insert, 或update的速率是會下降的, 因為他需要先找到, 才能插入或update, 但是沒做索引的話, 直接插入就好, 不用理會index的結構到底如何

		b. 與binary tree在效率上的差異:
			I. binary tree的效率為log_2(n), B-Tree的效率為log_m(n), 其中m為節點的degree分支度, 理論上log_m(n)和log_2(n)的order是一樣大的, 這個可以透過微積分證明, 但是通常我們如果資料量很大的時候, 使用硬碟去做資料結構, 我們會希望樹高越小越好, 因為硬碟加載數據效率較慢, 所以加載次數越少越好. reference:
				https://blog.csdn.net/weixin_44685869/article/details/106083874
				https://zhuanlan.zhihu.com/p/257842997


	E. B+ Tree (某種程度上可以看做是B tree的改良版本)
	reference:
	https://www.zhihu.com/question/57466414
		a. 與B Tree的區別:
			I. B+树改进了B树, 让内结点只作索引使用, 去掉了其中指向data record的指针, 使得每个结点中能够存放更多的key, 因此能有更大的出度. 这有什么用? 这样就意味着存放同样多的key, 树的层高能进一步被压缩, 使得检索的时间更短. 
			II. 当然了,由于底部的叶子结点是链表形式, 因此也可以实现更方便的顺序遍历, 但是这是比较次要的, 最主要的的还是第I点.

			
