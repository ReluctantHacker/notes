|#############################################################################################################|
1. some datastructure
	A. primitive: integer, float, string, boolean
	B. non-primitive: 
		a. 線性(in sequential order): list, tuple, array, linked list, stack, queue
		b. 非線性: set, dictionary, tree, graph

|#############################################################################################################|
2. Program Complexity
	A. Space complexity

	B. Time complexity
		a. f ∈ o(g)  ---   order(f)<order(g)  ---  lim f(x)/g(x) = 0 , x->∞
		a. f ∈ O(g)  ---   order(f)≤order(g)  ---  lim f(x)/g(x) < ∞ , x->∞
		a. f ∈ ϴ(g)  ---   order(f)=order(g)  ---  lim f(x)/g(x) ∈ R > 0 , x->∞
		a. f ∈ Ω(g)  ---   order(f)≥order(g)  ---  lim f(x)/g(x) > 0 , x->∞
		a. f ∈ ω(g)  ---   order(f)>order(g)  ---  lim f(x)/g(x) = ∞ , x->∞

|#############################################################################################################|
3. Recursive Algorithm(basic)
	A. Factorial
	B. Fibonacci Number
	C. Greatest Common Divisor(GCD)
	D. Binomial Coefficient
	E. Ackerman's Function
	F. Tower of Hanoi
	G. Permutation

	H. Recursive算法的大原則如下: 
	method_A: 由child到father進行運算, 也就是由所有leaf一路算回root (例子: binary tree的所有相關算法幾乎都是)
		def recursiveFunc():
			recursiveFunc()
			doSomething()

	method_B: 由father到child進行運算, 也就是由root一路算到所有leaf (例子: quickSort)
		def recursiveFunc():
			doSomething()
			recursiveFunc()

|#############################################################################################################|
4. array 
	A. 定義:
		a. array內的元素同type
		b. 元素間占用的記憶體是連續的
	B. 不同維度:
		a. first dimensional array
		b. second dimensional array
			a. 其實沒甚麼好講的, 有一點比較需要注意的是, c語言存儲array是採用"列為主(row-major, 就是你熟悉的)", 有些程序語言採用的"欄為主(column-major)", 以下有個比較清楚的說明:
			https://ithelp.ithome.com.tw/articles/10333963?sc=pt
		c. third dimensional array

|#############################################################################################################|
5. linked list
	A. 優點
		a. 動態性: 動態增加空間, 不需要一開始設定大小
		b. 非連續性: 不需要連續的儲存空間
		c. 在linked list任意位置插入元素, 或刪除元素無須將每一個元素移動(但注意, 你沒有該位置的位址, 仍要遍歷linked list才能訪問該元素, array則可直接藉由位置訪位該元素) : 時間複雜度O(1)(這是不算遍歷的)

	B. 缺點:
		a. 隨機訪問效率低: 需要從頭查找, 時間複雜度O(n), 若是array, 則只要指定位置即能直接訪問
		b. 占用額外的空間
	C. 補充:
		a. linked list其實可以看做是one branch tree, 也就是每個node都只有一個分支的tree
		b. tree的基本單位是node, linked list也是
		c. linked list與array是電腦裡面最基本的兩個資料結構, 兩者建構了其他的資料結構(在電腦實務科學上是這樣的, 因為記憶體的問題)
	D. Doubly linked list
		a. node裡面有*next也有*previous, 可以想像成singly linked list的高級版, 這樣便能同時知道這個node前後, 可以雙向回推, 更方便, 一般情況我們說linked list都是singly linked list
		b. stack 只需使用singly linked list, 因為stack只需操作一邊
		c. queue若不使用, 則需要用loop去找到數地的原數, loop會耗費效率, 在此使用doubly是更好的選擇(雖然會增加記憶體空間).  !!!!!   wait! I thought queue need doubly linked list. But now, I see that it doesn't! Because, first, insertion doesn't need any loop. Second, remove from front or rear, one of them needs loop. But hey, I can choose the one that doesn't need loop as the remove side of the queue! So queue doesn't need doubly linked list, either!
	E. Circular linked list
	reference:
	https://www.geeksforgeeks.org/linked-list-in-c/
    https://stackoverflow.com/questions/11867362/circular-queue-and-circular-linked-list
        a. circular linked list 可以用於建立double ended queue(他與circular array是平級概念), 且能無限制增長空間, 但同樣的無法像circular array一樣以O(1)的速度鎖定元素位置, 必須一路找下去. 記住linked nodes和array是電腦科學最基本的兩個data structure.  *我後來認為這樣的說法有點問題, 應該要說circular linked list可以用來實作queue, 而不是說他用來實作circular queue, 因為circular queue本身是指他是一個queue, 但記憶體的實作採用circular array的方式, 所以其實circular queue 就是queue, 只是強調了他用的記憶體實現方式而已
        b. Doubly circular linked list, 就好像doubly linked list可以用來實現double ended queue一樣, doubly circular linked list可以用來實現circular double eneded queue

	
|#############################################################################################################|
6. stack
	A. ADT
		a. 性質:
			I. First in Last out(FILO)

		b. 操作:
			I. push(): push an element in stack's top position
			II. pop(): return the the top element and remove it from the stack
			III. peek(): return the top element
			IIII. isempty(): return if stack is empty or not
			IIIII. size(): return how many data in stack
			ps: 基本你會發現stack push, pop 都是對top作用, 所以增刪都在同一個位置
	B. array 實作
		a. 實作要點:
			I. 一個array, 長度依需求
			II. 一個int top, 紀錄top的位置
			III. top的初始化為-1, push則+1, pop則-1

	C. linked list實作:
		a. 實作要點:
			I. 一個data
			II. 一個*next, 指向下一個node
			III. 初始node為NULL
			IIII. 與array實作的方向相反, 在header的地方push(也pop), 
			IIIII. header node代表了這個stack
			IIIIII. NULL node 代表這個stack的結束位置

|#############################################################################################################|
7. 中序式與(後序式 or 前序式)互轉的演算法
	A. infix -> postfix
		a. 需要1個stack
		b. 由左向右scan
		c. 遇到"數字"直接輸出至結果, 遇到"算符"則與stack的top比較優先級, 不能"<="top, 否則stack pop(), 直到不再"<=", 才push進stack, 注意! stack(stack只會需要存算符), 
		d. 遇到"("則push進stack, 遇到")"則pop stack, 直到遇到"("
	B. postfix的計算演算法
		a. 需要1個stack
		b. 由左向右scan
		b. 把"數字"push(pop)到stack(stack只會需要存數字)
	C. infix -> prefix
		a. 需要1個stack
		b. 由右向左scan
		c. 遇到"數字"直接輸出至結果, 遇到"算符"則與stack的top比較優先級, 不能"<"top, 否則stack pop(), 直到不再"<", 才push進stack, 注意! stack(stack只會需要存算符), 
		d. 遇到")"則push進stack, 遇到"("則pop stack, 直到遇到")"
		c. 由於是從右邊開始scan, 其結果會是reverse的, 所以你需要自己reverse過來(可以用另一個stack完成此事)
	D. prefix的計算演算法
		a. 需要1個stack
		a. 由右向左scan
		c. 把"數字"push(pop)到stack(stack只會需要存數字), 跟postfix很像, 只是計算時的次序要注意

	E. 重要練習題(裡面涵蓋一些概念, 細品):
		http://sptutor.dyu.edu.tw/DSTutor/exprDemo6.jsp


|#############################################################################################################|
8. Queue
	A. ADT
		a. 性質:
			I. First in First out(FIFO)

		b. 操作:
			I. enqueue(): push an element in queue's rear position
			II. dequeue(): return the the front element and remove it from the queue
			III. isfull(): return if queue is full or not
			IV. isempty(): return if queue is empty or not
			V. front(): return the front element of the queue
			ps: 基本你會發現queue push, pop 分別在頭和尾, 所以增刪在不同位置
	B. array 實作
		a. Linear Array實作要點(此法調整queue元素位置需要O(n), 不是很聰明):
			I. 一個array, 長度依需求
			II. 一個int front, 紀錄front的位置
			III. 一個int rear, 紀錄rear的位置
			IV. front, rear的初始化皆為-1, enqueue則rear++, dequeue則front++
			V. 當rear到最後一個位置時, 若front不是-1, 表示表示假滿, 把整個queue元素往左移動(front+1)格

		b. Circular Array(circular queue, circular buffer):
		reference: https://en.wikipedia.org/wiki/Circular_buffer
			I. 一個array, 長度依需求
			II. 一個int front, 紀錄front的位置
			III. 一個int rear, 紀錄rear的位置
			IV. front, rear的初始化皆為0, enqueue則rear=(rear+1)%n, dequeue則front=(front+1)%n. 詳細方法請參考講義或是程序實作
			V. circular method有分n-1及full-n法, n-1就是整個array有n個位置, 但只會用到n-1個位置, 此法是理論上速度最快的, 因為邏輯最為簡單, n法則是全部位置都用到, 但速度可能會較慢. 在底層世界(例如作業系統層級, linux等), 為了效率都會使用n-1法, 而"一個記憶體的位置"通常沒那麼貴, 是可以接受犧牲. 這部分是與chat-gpt討論的結果
            VI. Round-Robin(是使用circular循環概念很知名的例子), 例如假設有A, B, C三件事情要做, 則算法會A->B->C->A->B->C->A.....每次做的循環輪到某件事都會占用特定資源時間, 直到A, B, C都做完. 這其實就是CPU操作process的方式, 目的就是要使A, B, C看起來是同步執行的. 這便是Round-Robin, 其核心精神除了循環往復, 還有"每次循環的所有事件都輪流做一點點事情", 使所有事件看起來是同時推進的.

	C. linked list實作:
		a. 實作要點:
			I. 一個data
			II. 一個*next, 指向下一個node
			III. 需要一個front node和一個rear node, 兩個打包成一個queue
			III. 初始front node 和 rear node 皆為NULL
			IIII. rear node enqueue, front node dequeue
			IIIII. NULL node 代表這個queue的結束位置


|#############################################################################################################|
9. Priority Queue
	A. ADT
		a. 性質:
			I. 不一定First in First out(FIFO)
			II. enqueue任意元素(權值)
			III. dequeue最大/最小 權值的元素
            IV. 看起來, priority queue 與queue沒有任何關係, 他們insert和remove性質完全不同, 若是稱作priority stack似乎也說得過去, 我認為這個命名不是特別精準

		b. 操作:
			I. 利用heap tree(簡稱heap, 是min-heap或max-heap其中一種), 
				🤔. 定義
					😄. 為一complete binary tree(故必定平衡)
					😄. 父node必定比子node大(max heap, 反之則為min heap), 
					😄. 所有子node皆符合上述定義
			ps: heap和heap sort大有關係!!!!

			II. heap實作:
				🤔. 通常採用array的方式建構heap tree(不同於一般binary tree通常採用linked list), 這是因為heap 是complete binary tree, perfectly fit into arrays. 但不是不能採用node, 你仍需要會node的方法
                🤔. heapify有從上到下(移除最頂元素, 注意, 只能移除最頂元素!)及下到上(插入元素至最後位置)通常都是. 上到下的heapify會取最後一個元素至最頂的空缺, 這是為了使heap保持complete, 而下到上heapfiy只需往上比較parent, 直到比不贏為止.
                🤔. priority使會拉出最大值, 所以理論上不會出現移除heap首元素之外的元素, 但實際上heap的操作不局限於此, 在Dijkstra算法, 或A Search, Event Scheduling 中或有用處

            III. heap的優勢:
                🤔. 要實現priority queue 不一定非要heap, 有一個最直接的辦法, 就是enqueue的時候直接把數據照array的次序丟入, 這個enqueue只需O(1), 但是dequeue的時候, 需要一個for從array中選出最大(或最小)拿出, 然後在整組挪移, 需要O(n)
                🤔. 若是採用heap, 則enqueue要O(log(n)) 這是採用下到上的bubble up insertion.    dequeue也要O(log(n)), 採用heapify向下法, 這是樹狀結構的必然結果. heap的dequeue比基本array法有優勢.
            IV. Floyd's heap construction algorithm:
                🤔. 一個一個enqueue的時候通常採用bubble up的algorithm, 其單一耗時需O(log(n)), 建立n個元素的heap需要O(log(1) + log(2) + log(3) + log(4) + log(5)...) = O(log(n!)) = O(nlog(n). 
                🤔. Floyd's algorithm是一種一次把所有n個元素建立成heap的算法, 只需耗時O(n). 但他是一次性完成的, 一個一個就不行了. 以下是一個Floyd's algorithm之所以為O(n)的解釋, 很神奇
                https://stackoverflow.com/questions/9755721/how-can-building-a-heap-be-on-time-complexity
                    而且他其實就是一個等差-等比級數, 而等差-等比級數若n->infinity則其趨近於常數, 此稱之為Gabriel's staircase, 這使得最終耗時為O(n)
                https://en.wikipedia.org/wiki/Arithmetico-geometric_sequence
                🤔. 此法只適用把heap初始化的時候, 若需要之後再加入元素, 此法就不行了


	B. double ended queue(雙邊佇列)
		a. 性質:
			I. 兩端都可以自由插入或刪除(一般佇列是一端增, 一端刪)
			II. stack進出都同一邊. 而queue一邊進, 一邊出. Double Ended Queue (Deque) allows insertion and deletion on both sides. So, it is a combination of both Stack and Queue. 雖然命名為double ended queue, 但概念上也可叫做double ended stack. 他能模擬stack, 也能模擬queue, 且能做到兩者做不到的事.
			
		b. 實作:
			I. linked-list, 假設使用singly linked list, 由於singly linked list只支援單邊不用loop移除元素, 兩邊都要能移除元素的話, 一定有一邊要用loop, 為了效能, doubly linked list最適合實作.
			II. array, 
		c. 

	C. double ended priority queue(雙邊優先佇列)
		a. 性質:
		reference:
		https://www.geeksforgeeks.org/double-ended-priority-queue/
			I. A double ended priority queue supports operations of both max heap (a max priority queue) and min heap (a min priority queue) (簡單來說, 你能取得最大值, 也能取得最小值, 一般priority queue只能選一個)
			II. getMax() : Returns maximum element.
			III. getMin() : Returns minimum element.
			IV. deleteMax() : Deletes maximum element.
			V. deleteMin() : Deletes minimum element.
			VI. size() : Returns count of elements.
			VII. isEmpty() : Returns true if the queue is empty.

		b. 實作:
			I. 利用min-max heap:
			reference: (以下wikipedia有詳細解釋)
			https://en.wikipedia.org/wiki/Min-max_heap
				🤔. 定義:
					😄. The min-max heap property is: each node at an even level in the tree is less than all of its descendants, while each node at an odd level in the tree is greater than all of its descendants
					😄. 為一complete binary tree(故必定平衡)
					😄. The root element is the smallest element in the min-max heap
					😄. One of the two elements in the second level, which is a max (or odd) level, is the greatest element in the min-max heap 
				🤔. 操作:
					😄. insert(), 基本也是從最後一個節點插入, 然後往上比(push_up)
						😄. time complexity: O(log(n)), average and worst case
					😄. extract(), 基本也是往下比較, 兩個子節點的分支都要比較...記住min-max heap的定義, 就能發現如何處理.
						😄. time complexity: O(log(n)), average and worst case
	

	D. Symmetric Min-Max Heap(SMMH), 也可用在double ended priority queue
	reference:
	https://hackmd.io/@vRN1CwEsTLyHOsG4mC0d4Q/BkrYQ8B2V
	https://www.tutorialspoint.com/symmetric-min-max-heaps
		a. 定義:
			I. 每一個node的左子node, 都喊"我跟我的後代比起來, 我是最小的
			II. 每一個node的右子node, 都喊"我跟我的後代比起來, 我是最大的
			III. root的value為空

	E. deap (簡單版的SMMH), 也可用在double ended priority queue
	reference:
	https://medium.com/%E7%8B%97%E5%A5%B4%E5%B7%A5%E7%A8%8B%E5%B8%AB/%E5%9C%96%E8%A7%A3-double-ended-priority-queue-%E9%80%B2%E9%9A%8E%E6%A8%B9-1ae18d2ca402
		a. 定義:
			I. root為空
			II. root的左子tree, 為"min-heap"
			III, root的右子tree, 為"max-heap"
			IV. 在root左子上的任一點 i，令 j 為他在右子樹中對應的節點，i 點的權重必須小於 j 點的權重。
(p.s 若無對應點則取其父點)


|#############################################################################################################|
10. Binary Tree(Tree 也屬於 graph)
	A. 總性質:
		a. 任意tree可化為binary tree
		b. 任意node最多只有兩個degree(分支)
		c. 左右子樹有次序之分(一般tree沒有特別定義)

	B. Full binary tree
		a. 性質:
			I. 必定平衡
			II. 除了leaf以外, 所有node當成root的子樹都必定為full binary tree
			III. 最後一個level填滿
			IIII. 公式:
				🤔. N = 2^H - 1   等價於  log_2(N+1) = H, N=node數, H為高度
				🤔. lN = 2^(H-1)

	C. Complete binary tree
		a. 性質:
			I. 必定平衡
			II. 除了leaf以外, 所有node當成root的子樹也都必定為complete binary tree
			III. 最後一個level未必填滿, 但是一定由左到右填入, 不會有跳過
			IIII. 公式: log_2(N+1) = H, 無條件退位

	D. Pathological Tree
		a. 性質:
			I. H = N, H=高度, N=node數
		b. left-skewed binary tree
		c. rigth-skewed binary tree

	E. 實作
		a. 使用array
			I. 優點:
				🤔. 對於full B.T完全沒有浪費空間
				🤔. 容易取得某node之左右子node, 以及父node的資料
					😄. 某node的編號為i則:
						😄. 左子node編號為2i, if 2i>N, 左子node不存在
						😄. 右子node編號為2i+1, if 2i+1>N, 右子node不存在
						😄. 父node編號為[i/2]([]:無條件捨位), if [i/2]<1, 則父node不存在
			II. 缺點:
				🤔. 節點增刪不易, 若空間不構, 要重新宣告array
				🤔. skewed B.T極度浪費空間

		b. 使用linked list(大部分實作還是用這個)
			I. 優點: 
				🤔. node增刪容易
				🤔. 對於skewed B.T
			II. 缺點: 
				🤔. 不易取得父node
					😄. 因為每個node只存左子node以及右子node
				🤔. link空間仍浪費約一半
					😄. 因為所有leaf的link皆沒有用到
			III. 實作:
				🤔. 每個node皆需要
					😄. data
					😄. left child node(c語言可用指標)
					😄. right child node(c語言可用指標)
				🤔. root代表了這棵樹
		c. Tree Serialization(包含a提到的array法, 其實就是把complete tree灌入array中, 空的地方補#或null, 此法又叫做level-order), 還有常見的empty symbol preorder(其實還有postorder) Method. 其實level-order是breadth-first廣度優先循序的一個典型例子, 而preorder, inorder, postorder則都是depth-first深度優先的例子.
				🤔. 由於tree是二元以上的, 而serialiation本身是把二元的數據注入一元的資料格式中, 這種做法與數學上的高維用低微表示有關, 可以研究一下
			
	 
|#############################################################################################################|
11. Binary Tree Traversal(操作上幾乎都使用recursive)
	A. breadth first(廣度優先) Traversal  -----> sibling 先走, 通常需要使用queue
		a. 由左到右
		b. 再由上到下
		c. 簡單來說就是同一level的先處理完再處理下一個level

	B. Depth first(深度優先) Traversal  ----> 子node先走, 這本身就包含recursive的概念, 因為把子node當作root進行重複操作, 通常需要使用stack
		a. 前序(preorder, DLR) Traversal, preorderTrav()
			I. 採用recursive
			II. time complexity: O(n)

		b. 中序(inorder, LDR) Traversal, inorderTrav()
			I. 採用recursive
			II. time complexity: O(n)

		c. 後序(postorder, LRD) Traversal, postorderTrav()
			I. 採用recursive
			II. time complexity: O(n)

		d. 給(inorder和preorder)排序結果, 回推binary tree
			I. preorder的root必定在最前面
			II. inorder的root必定在左右子tree的inorder中間
			III. 重複I, II

		c. 給(inorder和postorder)排序結果, 回推binary tree
			I. postorder的root必定在最後面
			II. inorder的root必定在左右子tree的inorder中間
			III. 重複I, II

		d. 給(preorder和postorder)排序結果, 回推binary tree
			I. 可能有很多種結果, 故無法

		e. **空標記法**: 此法和以上的普通標記規則不大相同, 假設ABC為前序排序, 空標記法的規則明確說明B必定是A的左子節點, C必定為B的左子節點, 這點與普通標記法不同. 普通標記法之下ABC排序, B不一定是A的左子節點.  另外, 在空標記的規則之下, 可以(且必定)唯一決定樹的樣子, 例如ABC是可以唯一決定的, 例如更複雜的ABC##D#E(其中#為空節點), 直接按照規則嘗試建構樹則可知道.

		f. 計算B.T node 總數, count()
			I. 採用recursive

		g. 計算B.T 高度, height()
			I. 採用recursive

		h. B.T每一node左右子tree交換, swap()
			I. 採用recursive

		
|#############################################################################################################|
12. Binary search tree
	A. 定義:
		a. 左子樹所有Node鍵値均小於Root鍵値
		b. 右子樹所有Node鍵値均大於Root鍵値
		c. 左、右子樹亦是Binary Search Tree

	B. 建立(Insertion):
		a. 準備一段資料, 例如array
		b. 將array的首個資料當作root
		c. 接著按照binary search tree定義依序填入
		d. 平均時間複雜度: O(nlog n)
			I. 推導:
				🤔. total time complexity = log(1) + log(2) + log(3) + ... log(n) = log(n!)
				🤔. log(n!) = nlog(n) when n->∞, according to stirling formula(use intergral)
			若是不信, 以下還有一個討論:
			https://stackoverflow.com/questions/8118221/what-is-ologn-on-and-stirlings-approximation

		e. 最差時間複雜度: O(n^2)
			I. 剛我是一skewed tree
 
	C. 查找(search)
		a. 最壞情況O(n), 這棵樹剛好是skewed tree
		b. 平均情況O(log n)

	D. 插入(insert)
		a. 最壞情況O(n)
		b. 平均情況O(log n)
		c. 方法: 沒什麼好講的, 照BST定義

	E. 刪除(delete)
		a. 最壞情況O(n)
		b. 平均情況O(log n)
		c. 方法:
			I. 删除的节点是叶节点（没有子节点）：
				🤔. 直接删除该节点，不需要进行额外的调整。
			II. 删除的节点有一个子节点：
				🤔. 用该节点的子节点替代它。
			III. 删除的节点有两个子节点：
				🤔. 找到该节点的前驱或后继节点，用前驱或后继节点的值替换该节点的值，然后删除前驱或后继节点。前驱节点是左子树中的最大节点，后继节点是右子树中的最小节点。

	F. 排序(sort), BST也可以用於排序(但資料必須先建立成BST)
		a. 用inorder traversal, 即可得到"由小到大"的排序結果
		b. 時間複雜度自然是traversal的複雜度O(n)
		c. 在tree sort實作中, 需要把"create tree"的複雜度也算進來, 所以總共還是要O(nlog(n))

	
|#############################################################################################################|
13. Tree 或 forest 化 Binary tree
	A. Tree 化 binary tree
		a. sibling之間建立link
		b. 每一個node只保留最左子node的link和剛才建立的sibling link, 其餘link皆斷

	B. Forest(many unlinked tree) 化 binary tree
		a. 所以tree皆化binary tree
		b. 將binary trees們的root建立link
		c. 以第一顆tree的root為新root

	C. 其實Tree(多元)化binary tree本質與Forest化Binary tree是在做一樣的事情, 因為Tree化binary tree的過程, 實際上就是把每個節點的子樹集合化成一個binary tree, 然後遞迴到root去. 多做幾個範例就可以體會到其精神. 其實很多實際結構都類似Tree(多元), 這些多元tree其實後來都用二元tree的方式去處理了. 所以數學上tree化binary tree是很重要的


|#############################################################################################################|
14. 圖(Graph), 利用集合(set)的概念會很好理解
	A. 無向圖(undirected graph)
		a. G=(V, E), V=vertex, E=edge
		b. E通常表示(vi, vj), 其中vi, vj為兩相鄰vertex
		b. (vi, vj) != (vj, vi)

	B. 有向圖(directed graph)
		a. G=(V, E), V=vertex, E=edge
		b. E通常表示(vi, vj), 其中vi, vj為兩相鄰vertex
		c. (vi, vj) != (vj, vi)

	C. 完整圖(complete graph)
		a. 無向圖的完整圖
			I. 所有vertex都與其他vertex形成一個edge
			II. 若有n個vertex, 則共有n(n-1)/2個edge
		b. 有向圖的完整圖	
			I. 所有vertex都與其他vertex形成兩個(來回)有向邊edge
			II. 如有n個vertex, 則共有n(n-1)個edge

	D. 子圖(subgraph)	
		a. 集合V(G') 屬於 集合V(G) 且 集合E(G') 屬於 集合E(G), 則G'為G的subgraph

	E. 路徑(path)
		a. 定義:
			I. vertex v到v', 經過的所有edge的集合
		b. 長度(path length):
			I. edge的個數
		c. 簡單路徑(simple path):
			I. 除了頂點和終點, 其他點皆不同
				🤔. 若頂點和終點相同稱為"迴圈(cycle)"

	F. 連通(connected)
	reference: (以下參考是關於"連通"以及"連通分量(connected component)"的一些重要概念
	https://web.ntnu.edu.tw/~algo/ConnectedComponent.html
	https://web.ntnu.edu.tw/~algo/ConnectedGraph.html
		a. for 無向圖
			I. 任意vertex與任意vertex之間皆有path存在, 即表示此圖connected
			II. 若不連通, 則必定可以分成不相連子圖

		b. for 有向圖
			I. 強連通(strongly connected)
				🤔. 任意vertex與任意vertex之間皆有path(雙向)存在
			II. 弱連通(weakly connected)
				🤔. 任意vertex與任意vertex之間皆有path(單項或雙向)存在
				🤔. 但是至少有一對vertex之間只有單向path(只能往, 不能返)
			III. 不相連(disjoint)
				🤔. 並非任意vertex之間皆有path(單向或雙向)
				🤔. 圖可以分成不相連的子圖

		c. 連通元件(connected components):
			I. 子圖中的最大邊界連通圖(可能有好幾個), 通常一個無向圖如果好幾個部分不相連, 則這些不相連的子圖, 都是connected components

		d. 圖的"關節(articulation point)"
			I. 定義:
				🤔. A joint in a graph is a vertex whose removal increases the number of connected components.
		
		e. 圖的"橋"
			I. 定義:
				🤔. 與關節類似, 只是他是"邊"

		c. 雙連通(biconnected):
		reference:
		https://web.ntnu.edu.tw/~algo/ConnectedComponent.html
			I. 定義:
				🤔. for 無向圖: 任意一個節點被移除, 圖仍然是連通的, 稱此圖為"雙連通"
				🤔. for 有向圖: 一個雙連通的有向圖中，對於任何兩個頂點v和w，都有兩條從v到w的有向路徑，且除了v和w以外沒有其他公共頂點。

	
	G. 分支度(degree):
		a. for 無向圖
			I. 就是v有幾個e

		b. for 有向圖
			I. 入分支度(Indegree)
				🤔. 指向v的e個數
			II. 出分支度(Outdegree)
				🤔. 指出v的e個數
			III. 公式
				🤔. e = total_sum(indegree) = total_sum(outdegree)  (某點的入, 必定是某點的出!, 有入必有出)
				🤔. 從以上公式可以知道, 想要算總e, 只要算總indegree 或是 總 outdegree即可

	H. 資料結構(數學)如何表示graph?
		a. Adjacency Matrix(相鄰矩陣)
			I. for 無向圖(undirected graph)
				🤔. 此matrix為n*n, 縱向表n個nodes, 橫向也表n個nodes, 有link就1, 沒link就0
				🤔. 必為對稱矩陣
				🤔. (vi, vj)是否存在:
					😄. 檢查A[i][j]是否為1
						😄. 時間複雜度O(1)
			
				🤔. 求vi的e數:
					😄. 第i列 or 第i行 元素總和
						😄. 時間複雜度O(n) ---need 1 loop

				🤔. 求總e數:
					😄. total_sum(A[i][j])/2
						😄. 時間複雜度O(n^2)

			II. for 有向圖(directed graph)
				🤔. 矩陣的形式與無向圖一樣
				🤔. (vi, vj) <> (vj, vi), 故矩陣不一定對稱
				🤔. (vi, vj)是否存在:
					😄. 同無向圖
				
				🤔. 求vi的e數
					😄. Out-degree：求第 i 列之元素値總和。
						😄. 時間複雜度O(n) ---need 1 loop
					😄. In-degree：求第 i 行之元素値總和
						😄. 時間複雜度O(n) ---need 1 loop

				🤔. 求總e數:
					😄. total_sum(A[i][j]
						😄. 時間複雜度O(n^2)
		
		b. Adjacency List(相鄰串列)
			I. for 無向圖:
				🤔. 特性:
					😄. 跟linked list長的有點像, 但是意義完全不一樣, 不可搞混
					😄. 有n個node的graph, 用1維矩陣表示為A[i]
					😄. A[i] = vj|link -> vk|link -> vl|link........
					😄. A[i]存放的就是一個Adjacency List, 他表示與vi相連的vertex
					😄. Adjacency List的順序其實沒差啦 vj|link -> vk|link or vk|link -> vj|link都可
					😄. 此法所需node數(v|link)為2e (因為vi->vj, vj->vi都記錄下來了)
				
				🤔. (vi, vj)是否存在:
					😄. 檢查A[i]的adjacency list是否有node vj
						😄. 時間複雜度O(e/n) --- 這是平均和最差時間複雜度, 
							😄. 因為A[i]的adjacency list 長度正比於e
							😄. 平均而言len(A[i]) = e/n  (平均把所有node分散到每個A[i])
							😄. 你在長度e/n的串列做線性search, 自然複雜度O(e/n)
				🤔. 求vi的分支度(degree):
					😄. 求A[i]的adjacency list長度	
						😄. 時間複雜度O(e) ---
							😄. 因為要一個一個數, 所以跟複雜度search一樣

				🤔. 求graph的e數:
					😄. node總數/2
						😄. 時間複雜度O(n+e)
							😄. 找到A[i]需要O(1), traversal A[i]的adjacency list需要O(e), 所有處理一個A[i]總共要O(1+deg(vi))
							😄. 所有A[i]加總, O(1+deg(v1) + 1+deg(v2) + 1+deg(v3)... = O(|V| + |E|)
						😄. 當e為最大的時候e = n(n-1)/2, so 時間複雜度O(n+e) = O(n^2)
			
			II. for 有向圖(directed graph)
				🤔. 與無向圖幾乎一模一樣
	


|#############################################################################################################|
15. 圖(Graph)的traversal 
	A. 需設定每一個node的visited flag
		a. flag=0: 尚未拜訪
		b. flag=1; 拜訪中
		c. flag=2; 已拜訪

	B. 深度優先(depth first traversal)----> 子node 先走
		a. 過程
			I. 選擇一個起始vertex vi, 
			II. 選擇一個與vi相連接且flag=0的vj, 並設定vj為新的起始點
			III. 以vj作為新的起始, 重複上述步驟
			IIII. 直到找不到下一個flag=0的vertex 

		b. 實作方法, 由於DFT有"recursive"的概念在其中(子node當作新root), 故可用stack來保存走訪中的vertex
			I. 選擇一個起始vertex vi, push到stack中
			II. 若stack不為空
				🤔. 從stack中pop()出top, 視為已拜訪
				🤔. top相接的所有flag=0的vertex都push進stack  ---> 重複執行
				🤔. 若所有vertex都被拜訪過, 而stack仍不為空, 則stack清空
			III. 若stack為空, 則traversal 結束
			IIII. DFT的順序不unique, 除非你自己設定按照某種排列


	C. 廣度優先(breadth first traversal) ----> sibling node 先走(遍歷所有silbling)
		a. 過程:
			I. 選擇一個起始vertex vi,
			II. 選擇一個與vi相連接且flag=0的vj, 
			III. 重複上述步驟(注意! 這個時候起始點還是vi, 沒有變)
			IIII. 接著對剛才走訪過的sibling重複上述步驟
			IIIII. 直到找不到下一個flag=0的vertex

		b. 實作方法, 使用"queue"來保存走訪中的vertex, 同樣是recursive
			I. 選擇一個起始vertex vi, enqueue到queue中
			II. 若queue不為空
				🤔. 從queue中dequeue()出front, 視為已拜訪
				🤔. front相接的所有flag=0的vertex都enqueue進queue---> 重複執行
			III. 若queue為空, 則traversal 結束
			IIII. BFT的順序不unique, 除非你自己設定按照某種排列

	D. 用處:
		a. 判斷graph是否連通
		b. 可以找出連通的單元(組件, 或者說區塊吧)
		c. 起概念是"拓撲排序", "最短路徑"的基礎


|#############################################################################################################|
16. AOV(Activity On Vertex)  ---> 其實就是一個"有向圖"的應用
	A. 定義:	
		a. 其實就只是一個有向圖(directed graph), 不用一定要連通(兩個獨立的不互聯的節點也可以拓撲排序)
		b. vertex表示"工作(activity)"
		c. edge表示"工作的次數"(有方向嘛~)

	B. 拓撲順序(topological order, 或叫"拓撲排序") 
		a. 定義:
			I. 首先你的AOV圖不能有cycle
			II. 選一個起始vertex v_s和終點vertex v_e
			III. path v_s到v_e之間經歷的所有vertex(就是traversal!), 不能有vi->vj但是vj的排序在vi之前

		b. 卡恩演算法(精神其實就是, 每次都找沒有父node的節點, 因為這表示該節點的所有先導任務都完成了):
			I. 找出一個indegree=0的vertex
			II. 將此vertex輸出至result, 且刪除他的所有outdegree edges
			III. 重複I~II, 直到所有vertex接輸出至result, 或剩下的vertex都有indegree
			IIII. if 不是所有vertex都輸出至result, 則 沒有topological order
				🤔. 注意! AOV network圖若有cycle, 則必定沒有topological order(也就是無法決定那個工作先做)
				🤔. 注意! 不具cycle的AOV network圖, 則topological order >= 1組(不一定唯一)

		c. 經典現實例子: 
			I. 某校的選課系統中，存在這樣的規則：每門課可能有若干門先修課，如果要修讀某一門課，則必須要先修讀此課程所要求的先修課後才能修讀

	C. 資料結構如何表示?
		a. 使用adjacency list(不用adjacency matrix因為在這個case裡有點麻煩)
			I. 把A[i]改成一個2*n的矩陣A[c][i],  (就是多出一條來紀錄每個vertex的indegree數量)
				🤔. A[0][i] = count (vi的indegree數量)
				🤔. A[1][i] = vi的adjacency list

			II. 如何實作刪除vi的outdegree edge? (假設我們把vi丟到result, 同時也得刪掉她的outdegree edge才行)
				🤔. 假設A[1][i] = v1|link -> v2|link -> v3|link, 則
				🤔. A[0][1]--, A[0][2]--, A[0][3]--

				
				
|#############################################################################################################|
17. AOE(Activity On Edge, 就是常見的加權路徑圖)  ---> 也是就是一個有向圖的應用, 只是edge多了"加權值", 即加權圖
	A. 定義:	
		a. 其實就只是一個有向圖+edge權重
		b. vertex表示"事件(event)"
		c. edge表示"工作(activity"
			I. 有加權值, 通常表示所花費的time

	
	B. 應用:
		a. 求完成計畫藍圖所需的最少時間(最起碼), 即是求關鍵路徑(critical path, 必須建立在拓撲排序之上, 否則狗屁不通)
			I. 求start_event->end_event之最長路徑(就是Critical path, 臨界路徑)
				🤔. 為何是求最常, 這裡有點反常識, 一下有個說明
					https://zhuanlan.zhihu.com/p/340950042

					😄. 其實就是所有的路徑其實要同時執行, 所以最長時間的路徑才是影響結果的關鍵
	
		b. 如何縮短完成計劃藍圖的時間?
			I. 找到所有critical path的交集工作(即egde), 並盡可能縮短他的權重

	E. 數據結構:
		a. Adjacency Matrix:
			I. 與一般Adjacency Matrix差不多, 只是本來有邊是以1表示有無, 現在變成用edge的長度(權重)
		b. linked list
			II. 與一般Adjacency List 差不多, 只是linked list節點多存了"邊長(權重)"
				

		
|#############################################################################################################|
18. 延伸二元樹(Extended binary tree)  ---> 用在霍夫曼樹(Huffman's tree)
	A. 定義
		a. 大部分的時候我們會使用linked list來表示B.T, 但是, 這樣leaf的link就沒有使用到, 挺浪費!

		b. Extended B.T會在leaf link連上一些特定的東西, 來幫助運算, 也順便空間在利用
			I. leaf link連上的node稱之為 "External Node" 或是 "Failure Node(因為如果搜索資料到了此處, 表示根本沒有要找的資料, 故return fail)"
			II. 其餘原本的node稱值為 "Internal Node"
			III. 根據B.T的性質, 很容易可以得到num(External_Node) = num(Internal_Node)+1

	B. 一些常用概念:
		a. 若I=sum_i(path_len(root->vi)), vi為internal node
		b. 若E=sum_i(path_len(root->vx)), vx為external node
		c. 則E=I+2n,  其中n為internal node總數
		d. 又若n固定, 則E與I成線性關係
		e. 越是balance的tree, E與I就愈小
		f. 但若是external node有加權值得時候, 上一概念 不一定成立!!!!(huffman's tree不一定要平衡!!)

	C. Weighted External Path length(W.E.P.L, 加權外部路徑長度), 其實就是每一個External path都要乘上各自的weight
		a. 定義:
			I. 即是 W.E.P.L = sum_x(path_len(root->vx)*weight_x)

		b. 性質:
			I. 沒有加權的情況下, 越平衡的樹, 搜索效率會越高, 總路徑長度(同時...平均路徑長度)會越短
			II. 但是有加權過後, 就不一定了

		
	D. 給定n個weight(也就是一定有n個external node), 如何求Min. W.E.P.L,  以及他對應的tree(就是huffman's tree了)
		a. Huffman's Algorithm: (與shannon theory裡面的entropy 大有關係, 但先不討論...)
			I. 設W為給定的weights的集合
			II. 算法如下:
				🤔. 將W中所有weight都設定為external node
				🤔. 自W中取出兩個最小的權重並相加(wi+wj), i
				🤔. 將(wi+wj)設定為wi和wj的父internal node
				🤔. 將(wi+wj)放入W中
				🤔. 重複上述步驟, 直到W中只剩一個weight(即是這顆huffman tree的root)

			III. Huffman Algo的精神:
				🤔. weight越大越應該離root近
				🤔. 從最小的weight開始建tree, 可以使大的weight更靠近root

			IV. 時間複雜度:
				🤔. building huffman tree: O(Nlog(N))
				🤔. Encoding O(N)
				🤔. Decoding O(N)

			V. 經典應用:  編碼/解碼 的最佳情況
				🤔. 要傳遞一段英文訊息, 共有26個英文字母代號可用, 每個字母出現的頻率已知, 若要使用2進制(0, 1)來為26個字母編碼, 怎樣會節省空間, 最有效率?
					😄. 這個問題中, 字母頻率其實就是weight
					😄. Huffman tree root往左走代表0, 往右走代表1
					😄. 如此的編碼將會在平均上使傳遞的訊息 用最少的bit來表示, 最節省空間(因為他用了"min W.E.P.L", 同時編碼/解碼正比於bit數 所以 在平均上 也會達到最快(不可能更快了, 這跟信息entropy理論有關)
					😄. 其精神便是越常出現的字母, 其編碼使用的bit數要越少(即path長度越短)

			VI. 程式實作: 
			reference:
			以下包含了python, c, java, c++(除了python以外都很麻煩)
			https://www.programiz.com/dsa/huffman-coding

				🤔. 由於需要"選取最小的權重出來", 你會需要"priority queue", 所以用通常會使用到min-heap來實作
				🤔. 設計步驟: (較難理解, 最好搭配code一起看)
					😄. 先用array的辦法build出一顆min heap tree, 此heap的每一個node都存"一個huffman tree node"
					😄. 這裡比較繞...此huffman tree是一個linked list, 所以她的root代表了這棵樹, 
					😄. 而huffman tree的root就是按照huffman algorithm, 把heap全部的node都extract之後最後一個node
				

|#############################################################################################################|
18. 自平衡二元搜索樹(self-balancing binary search tree)----> binary search tree的理論延伸 
	reference:
	https://zh.wikipedia.org/zh-tw/%E5%B9%B3%E8%A1%A1%E6%A0%91

	A. 目標:
		a. 我們知道binary search tree越是balance, 平均search效率就越高: log(n)--->事實上是最高了...
		b. 但我們在建構binary search tree的時候, 不見得會得到一顆平衡的樹, 我們需要些辦法獲得平衡的BST
		
	B. 常見的算法(方法), 大部分都使用了"旋轉(rotate)"的操作:  每次insert都需要log(n)複雜度+1個rotate: O(log(n)
		a. AVL tree(AVL是人名縮寫) 
			I. 定義:
				🤔. 為一顆binary search tree, 且height 平衡(balance)
				🤔. 嚴格定義平衡(balance):
					I. |height(Lchild(root))-height(Rchild(root))| <= 1,  
						😄: height(Lchild(root)) 為 左子tree高度, 
                                                😄. height(Rchild(root)) 為 右子tree高度
				🤔. 左右子tree也是AVL tree

			II. 平衡因子(Balance factor, BF):
				🤔. BF = h_l-h_r
				🤔. 在AVL tree中, BF只可能是: -1, 0, +1

			III. 建構時出現的不平衡情況:
				🤔. LL不平衡: 
					😄. height(Lchild(root))-height(Rchild(root)) > 1, 
					😄. 且height(Lchild(Lchild(root))) > height(Lchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) > 1
					😄. 且height(左子tree的左子tree) > height(左子tree的右子tree)
				🤔. LR不平衡
					😄. height(Lchild(root))-height(Rchild(root)) > 1, 
					😄. 且height(Lchild(Lchild(root))) < height(Lchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) > 1
					😄. 且height(左子tree的左子tree) < height(左子tree的右子tree)
				🤔. RL不平衡
					😄. height(Lchild(root))-height(Rchild(root)) < -1, 
					😄. 且height(Rchild(Lchild(root))) > height(Rchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) < -1
					😄. 且height(右子tree的左子tree) > height(左子tree的右子tree)
				🤔. RR不平衡
					😄. height(Lchild(root))-height(Rchild(root)) < -1, 
				😄. 且height(Rchild(Lchild(root))) < height(Rchild(Rchild(root)))
					😄. 翻譯成人話: height(左子tree) - height(右子tree) < -1
					😄. 且height(右子tree的左子tree) < height(左子tree的右子tree)

			IIII. 調整不平衡的算法, 採用旋轉的方法:  ----> 快速公式(適用AVL tree)
				🤔. 其實你根本不用管是哪種不平衡, 只要記住以下
					😄. 最大的值擺在R_child, 最小的值擺在L_child, 中間值擺在root
					😄. 若發生不平衡的插入值不再以上3個值之內, 則此插入值重插
				🤔. 調整的時候會平衡所有子node, 不會對父node造成任何影響, 所以理論上, 你只需要選轉一次即可(重要概念!!考過!)

			V. 旋轉詳細說明:   ----> AVL解題時用之前的快速公式即可, 但以下的概念還是十分有用, 要會
			reference: 以下有個比較清晰的參考說明:
			https://simpletechtalks.com/avl-tree-self-balancing-rotations-right-left-rotation-explained/
				🤔. 若發生RL不平衡, 則在先從較低level的開始調整, L不平衡的部分的連線元素, 進行順時針旋轉(稱之為右旋right rotation), 然後對剩餘L不平衡的部分(較高level)的連線元素, 進行逆時針旋轉(稱之為左旋left rotation).  發生LR不平衡, 則倒過來做. 若發生RR不平衡則只需對高level元素做left rotation.  LL不平衡則只需對高level做right rotation

			VI. remove node:
			reference:  --->以下有很清楚的說明, 就是移植不斷往下平衡
			https://www.cs.emory.edu/~cheung/Courses/253/Syllabus/Trees/AVL-delete.html
				🤔. 從AVL樹中刪除，可以通過把要刪除的節點向下旋轉成一個葉子節點，接著直接移除這個葉子節點來完成。因為在旋轉成葉子節點期間最多有log n個節點被旋轉，而每次AVL旋轉耗費固定的時間，所以刪除處理在整體上耗費O(log n) 時間。

			VII. 高度-最少節點數定理
				🤔. 建立高度h的AVL tree, 最少需要多少節點?
					😄. 答: S(h) = S(h-1) + S(h-2) + 1, 原理如下:
						😄. 首先, AVL tree的高度為h, 則任意node的左右子樹應為一個高度為h-1, 另一個高度為h-2, 這是最少節點的時候必定符合的
						😄. 所以分別計算左右子樹的最少節點, 然後相加: S(h-1) + S(h-2), 最後在加上本身自己也算一個節點, 所以S(h)為 S(h-1) + S(h-2) + 1, 遞迴~
						😄. 只要知道S(1)=1和S(2)=2就能計算所有的S(n)了

					😄. 答2: S(h) = F(h+2) - 1,   F(n)為費氏數列第n項
						😄. 證明較難, 先跳過

				🤔. 範例1:
					😄. 高度3的AVL tree, 至少要Fibonacci(3+2)-1 = Fibonacci(5)-1 = 4個節點
					😄. 高度3的AVL tree, 最多要?
						😄. 就是盡量建立成full tree, 所以答案是 2^h - 1 = 7

				🤔. 範例2:
					😄. 一個AVL tree有15個nodes, 則他的最大高度為?
						😄. Fibonacci(h+2)-1 <= 15 < Fibonacci(h+2+1)-1
							😄. 答: h = 5
					😄. 一個AVL tree有15個nodes, 則他的最小高度為?
						😄. 就是儘量建立成full tree, 答案: log(n+1)=4 (無條件退位, 跟complete tree的高度算法是一樣的)
		
		b. 樹堆(Treap)

		c. 伸展樹(Splay tree)
	
		d. 紅黑樹(Red-black tree)
		reference:
		https://tigercosmos.xyz/post/2019/11/algorithm/red-black-tree/
		https://www.youtube.com/watch?v=UaLIHuR1t8Q
			I. 定義:	
				🤔. root必為黑
				🤔. root到葉節點的任意路徑不能出現連續兩個紅
				🤔. 從root到任意只有一個children的節點, 其路徑上的黑色節點數量必定彼此相同
			II. Insertion
				🤔. 若為空, 則插入黑色節點為root
				🤔. 不為空, 一律插入紅色節點
					😄. 若該紅色節點的parent為黑, 則ok
					😄. 若該紅色節點的parent為紅, 違反紅黑樹定義, 需要調整
						😄. 若該節點的parent的sibling為紅
							😄. 首先我們為了讓紅不要連續, 我們把parent紅轉黑, 
							😄. 因為此時parent的sibling也為紅, 則為了使定義第三點成立, parent的sibling也要轉黑, 黑色數量才會對
							😄. 但是此時parent和parent silbing兩個分支都多了一個黑色節點, 這表示與其他非這兩分支的分支比較起來, 多了黑色數量, 這與第三點定義又矛盾了, 為了修復, 你必需在把這兩個parent的parent顏色也改變, 才能確保符合定三點定義(如果改變過程又出現RR連續, 你重複相同操作步驟一路刷到root)
						😄. 若該節點的parent的sibling為黑或為空, 則該節點rotate且轉黑
							😄. 首先, 因為我們一樣為了讓紅色不要連續, 把parent紅轉黑, 
								😄. 此時若parent的sibling為空, 則表示你無法藉由改變parent的sibling的顏色來使結構滿足定義三, 此時唯一的辦法就是使用rotate, 一樣大擺右, 小擺左, 中間擺中間, 接下來依照定義自由發揮了吧. (或利用AVL tree的概念RL不平衡就用R旋轉+L旋轉)
								😄. 此時若parent的sibling為黑, 則表示你依舊無法藉由改變parent的sibling的顏色來使結構滿足定義三, 因為parent的sibling本來就是黑色, 你改成紅色反而讓黑色數量減少了, 而parent改成黑色後自己的分支黑色數量增多, 所以違反定義三.   此時唯一的辦法就是使用rotate, 一樣大擺右, 小擺左, 中間擺中間, 接下來依照定義自由發揮了吧. 
		e. 加權平衡樹(weighted balanced tree)


						
|#############################################################################################################|
19. m-way search tree (binary search tree的擴展)   *****---- degree>2的樹, 高考好像沒考過*****
	A. 定義:
		a. 每個node可以有多個key, 但key的數量不超過(m-1)個, 但也至少要有1個key
		b. 每個key各有兩個degree(左右)
		c. 根據以上, 可以得到一個結論: 每個node的degree有m個, 且至少2個(你至少有1個key)
		d. node的key由小排列到大
		e. 各node的子tree同樣是m-way search tree
		f. 子tree內的所有資料皆小於他的父key
		
	B. 目的:
		a. 資料量龐大的時候, binary search tree高度太大(搜索慢), 如果node的degree多一點, 可以有效降低高度
			I. 外部搜索(External Search)
				🤔. 把資料放在記憶體以外的儲存空間進行處理
			II. 內部搜索(Internal Search)
				🤔. 把資料放在記憶體處理, 但是空間很有限, 資料量不能太多


|#############################################################################################################|
20. B-tree           
	A. 定義:
		a. 即是一個balanced的m-way search tree! 
		b. 除了root及failure node之外, 其餘node的degree介於[m/2]至m  ----> 重點!!!
		c. 所有failure node在同一個level, 所有leaf在同一個level

	B. 最簡單的B-tree:    2-3 tree:
		a. 就是m=3(你如果m=2就變成balanced B.S.T), 根據定義任意node的degree介於2到3

	C. 操作
	reference:  below is a very good example talks about how to insert and delete
	https://www.tutorialspoint.com/data_structures_algorithms/b_trees.htm
		a. Insert:
			I. 過程可能會遇到node裡面key數量超過m-1的狀況(overflow), 需要"split"
				🤔. split的辦法:
					😄. 將第[m/2]key 拉到父node, 其餘keys拆分成左右2個node
					😄. 這個辦法能保證你的B-tree一直是B-tree

		b. delete:
			I. 先找到要delete的鍵值, 假設為x
			II. 過程可能遇到node裡面key數量低於[m/2]-1(underflow), 這個時候需要"rotate"或"combination", 以下分為2種狀況:
				🤔. x在leaf:
					😄. 檢查拿掉x後有無underflow
						😄. 若無則OK
						😄. 如有
							😄. 嘗試rotation
							😄. rotation 不可行, 則做combination
							😄. 接著檢查combination完後, 父node有無underflow
							😄. rotation, combination的發法請參考講義(有點複雜)
							😄. rotation, combination的大原則如下:
								😄. rotation: 若x node underflow, 則其sibling送key給父node, 父node在送key給x node
								😄. combination: 若x node underflow, 且其sibling沒有key可送, 則父node送key給x node, 並把x node與其他sibling合併

				🤔. x在非leaf node:
					😄.  以x的右子樹中之最小鍵値取代x；或
					😄.  以x的左子樹中之最大鍵値取代x
					😄. 左右子樹都沒人可以借, 則x的parent和他所有的chilren combine.

	D. 應用
		a. 資料庫索引(index)
			I. 其採用的辦法正是B-Tree資料結構, 
				原理: 為何可以增加搜索數據的速度, 其實不難理解, 資料存入資料庫若是沒有任何資料結構, 那搜索將採用線性搜索, 也就是時間複雜度是O(n), 如果做了B-Tree的索引結構, 那搜索速率自然提升到O(log(n)) 補充: 雖然索引(index)會使搜索效率提升, 但是insert, 或update的速率是會下降的, 因為他需要先找到, 才能插入或update, 但是沒做索引的話, 直接插入就好, 不用理會index的結構到底如何

		b. 與binary tree在效率上的差異:
			I. binary tree的效率為log_2(n), B-Tree的效率為log_m(n), 其中m為節點的degree分支度, 理論上log_m(n)和log_2(n)的order是一樣大的, 這個可以透過微積分證明, 但是通常我們如果資料量很大的時候, 使用硬碟去做資料結構, 我們會希望樹高越小越好, 因為硬碟加載數據效率較慢, 所以加載次數越少越好. reference:
				https://blog.csdn.net/weixin_44685869/article/details/106083874
				https://zhuanlan.zhihu.com/p/257842997


	E. B+ Tree (某種程度上可以看做是B tree的改良版本)
	reference:
	https://www.zhihu.com/question/57466414
		a. 與B Tree的區別:
			I. B+树改进了B树, 让内结点只作索引使用, 去掉了其中指向data record的指针, 使得每个结点中能够存放更多的key, 因此能有更大的出度. 这有什么用? 这样就意味着存放同样多的key, 树的层高能进一步被压缩, 使得检索的时间更短. 
			II. 当然了,由于底部的叶子结点是链表形式, 因此也可以实现更方便的顺序遍历, 但是这是比较次要的, 最主要的的还是第I点.

			
