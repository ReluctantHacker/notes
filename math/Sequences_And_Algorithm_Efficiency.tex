\documentclass{article}
\usepackage{amsmath}


\title{The Study of Sequences and the Efficiency of an Algorithm}
\author{Jeffrey}
\date{2024/xx/xx}


\begin{document}
\maketitle
  \section{Introduction}
  In this study, I am going to take a deep look about Sequences and Series, what makes them Converge and their rate of Convergence. I should clealry understand the concepts of Sequences, Series, and their relationships with each other. This understanding is crucial because Sequences and Series form the foundation of the Calculus. The convergence of a Sequence is actuallly a part of the definition of differentiation.

  \section{What is a Sequence?}
  When discussing sequences, it's essential to consider the concept of a 'set' as well; they are closely related. A sequence can be thought of as a set without order. (In this study I only talk about discrete sets and sequences but there's actually continuous ones!) \\

  In 'Naive Set Theory', the elements in a set can encompass anything; for instance, let $S_1=\{A, B, C, D\}$, where A could represent a person, B might signify an abstract concept like a president falling in love with a little girl, C could be a pizza, and D an integer. The possibilities are entirely open. \\

  The key distinction is that a set, such as $S_1=\{A, B, C, D\}$, is indifferent to the order of its elements. It is identical to $S_2=\{B, C, D, A\}$; the order doesn't matter. However, in a sequence, the order is crucial. For example, Sequence $C_1=\{A, B, C, D\}$ is NOT the same as Sequence $C_2=\{B, C, D, A\}$. \\

  The elements in a sequence can take various forms, just like elements in a set. However, in this study, I specifically focus on "Sequences of numbers", encompassing real numbers, irrational numbers, imaginary numbers, and so forth. \\ 

  There are several well-known sequences, such as Fibonacci Sequence, Bernoulli Numbers, Natural numbers, Prime numbers and others. These sequences may be finite or infinite but all have their own specific arrangement order or pattern. Identifying patterns within these sequences allows for the 'prediction' of the next element in a certain sense. If a sequence lacks a discernible pattern, it is categorized as a random sequence, which holds significance in probability theory and statistics. This study exclusively delves into sequences with identifiable patterns. \\

  \section{Definition of Limit}
  Now that we have a fundamental understanding of what sequence is, let's skip the easy part and delve directly into the infinite situation. A sequence can consist of an infinite number of terms. In such cases, our curiosity is often piqued about the behavior as we approach infinity. If the sequence converges to a specific number, we classfiy it as a convergent sequence. If the sequence does not converge to a specific number, such as reaching infinity, we refer to it as a divergent sequence.  \\
  
  \subsection{The weird Infinitesimal in Differentiation}
  These things are easy but it is actually powerful and is used in differetiation. For example, how do we solve this $\frac{d(x^2+x+1)}{dx}$?  It's simple. \\

  \begin{center}
	  $\dfrac{d(x^2+x+1)}{dx} =$ \\[1ex] 
	  $\dfrac{[(x+dx)^2+(x+dx)+1]-(x^2+x+1)}{dx} =$ \\[1ex]
	  $\dfrac{x^2+2xdx+dx^2+x+dx+1-x^2-x-1}{dx} =$  \\[1ex]
	  $\dfrac{2xdx+dx^2+dx}{dx} =$ \\[1ex]
	  $2x+dx+1 =$ \\[1ex]
	  (For dx, which is infinitely small compared to 2x and 1, should counts as 0.) \\[1ex]
	  $2x+1$ \\[1ex] 
  \end{center}

  \subsection{The Second Crisis of Math}

  In the above example, of course, one might just consider applying Newton's binomial theorem; however, not this time. We see, we treat dx as a real number in the process at the beginning, but in the end we treat it as 0. This is actually a significant issue here, as dx should not simultaneously be a real number and 0. The root of this problem lies in the lack of a clear definition for dx, which is an infinitely small number referred to as 'infinitesimal'. \\

  This issue is famously known as the 'Second Math Crisis', which occurred during the era of Newton and Leibniz. There are two ways to deal with this infinitely small crisis. First way, as one can intuitively come up with, is to define dx rigorously. This appoach is known as 'Non-Standard Analysis(NSA)', mainly developed by Abraham Robinson around 1960. However, the reason we refer to this intuitive solution 'Non-Standard' is that it was developed after the 'Standard analysis', which uses 'limit' idea instead of 'infinitesimal'. Most mathematicians accept 'Standard' more than 'Non-Standard' nowaday ,and interestingly, it may be more intuitive. \\

  The reason I said it may be more intuitive is because nobody truly understand what 'infinitely close' or 'infinitely long' means, but everybody understands the idea of "wherever you go, I can go closer". It uses a Trick to make infinite question collapse to a finite question. \\

  The definition of 'limit' was developed by great Cauchy and Weierstrass. For example, the limit of a function $f(x)$ as $x$ approaches $c$ is denoted by 

  \[
	  \lim_{x \to c} f(x)
  \] 

  and represent the value that $f(x)$ approaches as $x$ gets arbitrarily close to $c$. 

  \[\lim_{x \to c} f(x) = L\]

  We can use the famous Epsilon-Delta definition to define what 'limit' reallly is:

  \[\forall \epsilon > 0, \exists \delta > 0 \text { such that } 0 < |x -c| < \delta \implies |f(x) - L| < \epsilon\]

  $\epsilon$ and $\delta$ can be any number. That means you can always find another smaller one. See, instead of directly dealing with 'infinitesimal', it says for all $\epsilon$ . This 'all' idea replaces 'infinity' which we don't really understand. Very clever. \\

  People developed this concept around age 5 or maybe 6 when playing 'Say The Biggest Number' on the playground(around the time you are leanring words like 'thousand', 'million', 'billion' etc). Eventually someone comes up with "Whatever you say plus one". \\

  This idea, "Whatever you say...I adjust it thusly..." is incredibly deep and is the foundation for all rigorous ideas of 'limit'.  \\

  It is interesting because it places the work of infinity on your imaginary opponent. They actually give you epsilon out of this weird, murky infinite soup... and after that its a normal finite problem to manipulate it in your argument. \\


  (If you are interested in digging deeper into 'Naive Set Theory' and 'Continuity of sets', you can check the study I create 'The Third Math Crisis: A Path from Naive Set Theory to Modern Set Theory.'. Here are some references:\\
  https://math.stackexchange.com/questions/3299369/is-an-uncountable-set-and-a-continuous-set-the-same-thing \\
  https://mathbitsnotebook.com/Algebra2/FunctionGraphs/FGContinuousDiscrete.html \\
  https://zhuanlan.zhihu.com/p/60177203 \\
  ) \\


  \section{The Infinite sequences and its convergence}
  Now we understand the idea behind how differentiation works, it takes the idea of 'Convergence of Sequences' and 'for all'. Let's keep talking about convergence of a sequence. We focus on the discrete sequences here.(However, remember there are continuous ones.") 

  If a sequence

  \[
	  \lim_{n \to \infty} a_n = C \text{ , for }n=1, 2, 3...\infty
  \] 


  If C is a finite real number, We say this sequence is convergent to C or else, if C is $\infty$, $-\infty$ or uncertain we say it is divergent. 

  In the most cases, if you find the function of $a_n$, like $a_n = f(n)$ then you can know if it's convergent or not. For example, a geometric sequence is like this below

  \[
	  a_n=f(n)=ar^{n-1}
  \]

  follows the recursive relation
  
%  \begin{center}
%  	$a_n=ra_{n-1}$ for every integer $n\geq2$
%  \end{center}

  \[
	  a_n=ra_{n-1} \text{ for every integer } n\geq2
  \]

  Such sequence is obviously divergent for 

  \[
	  \lim_{n \to \infty} ar^{n-1} = \infty
  \]

  For another example, Fibonacci sequence

  \[
	  F_n = \{1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\} = F(n) = \dfrac{\phi^n - \psi^n}{\phi - \psi} = \dfrac{\phi^n - \psi^n}{\sqrt{5}} \text{ where}
  \]

  \[
	  \phi = \dfrac{1+\sqrt{5}}{2} \approx 1.618... \text{ which is the well-known golden ratio, and }
  \]

  \[
	  \psi = \dfrac{1-\sqrt{5}}{2} \approx -0.618... \text{ } \phi \text{ and } \psi \text{ are conjugate to each other.}
  \]

  And of course, it follows the recursive relation

  \[
	  F_n = F_{n-1} + F_{n-2}
  \]

  Fibonnaci sequence is divergent and it's term tend to infinity. The sequence of ratios of n-th and n-1 term of Fibonnaci sequence is Convergent.

  \[
	  \lim_{n \to \infty} F_n^* =
  \]

  \[
	  \lim_{n \to \infty} \dfrac{F_{n+1}}{F_n} = 
  \]

  \[
	  \lim_{n \to \infty} \dfrac{\phi^{n+1} - \psi^{n+1}}{\phi^n - \psi^n} = 
  \]

  \begin{center}
	  (Because $\displaystyle \lim_{n \to \infty} \psi^n = 0$)
  \end{center}

  \[
	  \lim_{n \to \infty} \dfrac{\phi^{n+1}}{\phi^n} = \phi = 1.618...
  \]

  It converges to the well-known golden ratio.

  \section{The Rate of Divergence or Convergence of a sequence}
  A sequence can be convergent or divergent, of course; However, if there are two sequences which have the same starting point and ending point(which can be $\infty$, $-\infty$ or a real number $L$,), then, which one is going faster? That means which one need less steps to get closer to the infinity point?

  \subsection{The definition of this so called "Rate"}

  For example, 

  \[
	  A_n = n \text{ ,} A_1 = 0 \text{ ,} A_\infty = \infty
  \]

  \[
	  B_n = n^2 \text{ ,} B_1 = 0 \text{ ,} B_\infty = \infty
  \]

  $A_n$ and $B_n$ both start at the $0$ and diverge to the $\infty$. Well, Apparently, $B_n$ grows more faster than $A_n$, right? But wait... what does this "faster" even mean? We need a rigorous definition of this 'growing speed'. The reasonable definition should be like this: 'when n plus 1 how much this sequence's term grows?' And this is differentiation. So, we can have this 

  \[
	  A_n = A(n)
  \]

  \[
	  B_n = B(n)
  \]

  We can easily see that 
  
  \[
	  \dfrac{dA(n)}{dn} = A'(n) = 1
  \]

  \[
	  \dfrac{dB(n)}{dn} = B'(n) = 2n
  \]

  And, $B'(n)>A'(n)$ for $n>0$. So, we can say that $B(n)$ grows faster, or in other words, $B(n)$ get closer to $\infty$ more faster than $A(n)$ does for $n>0$. \\

  With this basic idea above, we can compare sequences by their differential, all we need to do is just finding the sequence's function. \\

  Let's just hold a second and talk about another closed math object: Series

  \subsection{What is a Series?}
  A Series is either finite or infinite, just like a sequence is. In the end, we all know that a series is just a summation of every term in a sequence. Nothing more, right? Interestingly, there's still something more can be discussed. \\

  Specifically speaking, if we collect all the Series of a summation of a sequence, then we get a Sequence. How? See, we could have

  \[
	  \sum_{n=1}^{m} A_n = S_m
  \]

  $S_m$ is actually a sequence based on m and it's parent sequence! \\

  Now when the $m \to \infty$, then we get a infinite series

  \[
	  \lim_{m \to \infty} \sum_{n=1}^{m} A_n = \lim_{m \to \infty} S_m
  \]

  In sum, it's all about sequences. A series is just a sequence of additions, or a sequence of partial sums; conversely, every sequence of addable objects(in particular, numbers), can be decomposed into a series.

  \subsection{Digging deeper about The Rate of Divergence or Convergence}
  Now, with the basic concept about Series, we discuss some famous series. First, the Harmonic Series

  \[
	  \sum_{n=1}^{\infty} \dfrac{1}{n} = 1 + \dfrac{1}{2} + \dfrac{1}{3} + \dfrac{1}{4} + ...
  \]

  It seems like the series is convergent because $\displaystyle \lim_{n \to \infty} \dfrac{1}{n} = 0$. But no! We can easily show that this series is actually divergent to $\infty$ by using some convergent test(like integral test), though the convergent rate is very slow. (In fact, it is so slow that we need to sum all the $10^{43}$ terms to make the series get $100$ but it really does diverge.) \\

  How do we exactly find the convergent rate of this harmonic series? Just like the conclusion we just got, we should find the sequence of this series, which is $H_m$ 

  \[
	  H_m = \sum_{n=1}^{m} \dfrac{1}{n}
  \]

  This sequence, we also call it 'Harmonic Numbers'. \\

  Harmonic numbers have been studied since antiquity and are important in various branches of number theory. They are sometimes loosely termed harmonic series, are closely related to the 'Reimann zeta function', and appear in the expressions of various special functions. We can check more details in Wikipedia. The specific form of Harnonic numbers is not that simple to get, the first accurate form is derived by Great Euler; However, we don't need the form of Harmonic numbers to get the divergent rate of it because we have a powerful weapon: Integral. How does integral change the game? See, we have the differential of Harmonic numbers

  \[
	  \dfrac{dH_m}{dm} =
  \]

  \[
	  \dfrac{d \left( \displaystyle \sum_{n}^{m} n^{-1} \right)}{dm}
  \]

  Now what? We don't know the form of $H_m$, it should be a function $H(m)$. Don't worry, we actually don't need it here, integral does the trick. You see, when $m \to \infty$

  \[
	  \lim_{m \to \infty} \sum_{n}^{m} n^{-1} = \int_{1}^{m} n^{-1} dn
  \]

  This 'integral' idea is very much like the idea 'Integral Test'. It can be used in most cases, and would lead us to a very simple result.

  \[
	  \dfrac{d \displaystyle \int_{1}^{m} n^{-1} dn}{dm} = 
  \]

  \[
	  \dfrac{d \left( \ln n |_{1}^{m} \right)}{dm} = 
  \]

  \[
	  \dfrac{d (\ln m - \ln 1)}{dm} = 
  \]

  \[
	  \dfrac{d (\ln m)}{dm} = 
  \]

  \[
	  \dfrac{1}{m} = m^{-1}
  \]

  And we see the result is exactly the m-term in the original series $\displaystyle \sum_{n}^{m} n^{-1}$. Interesting, right? We just go around and turn back. Here's more, this result can generize to any series because in the process above we first integral then differentiate it and the starting point is constant so the result must be the term itself. In the end, we have this conclusion:

  \begin{center}
	  \textbf{For a series $A = \displaystyle \sum_{n}^{\infty} a_n$ , then it's rate of divergence(and also convergence) is $\displaystyle \lim_{n \to \infty} a_n$}
  \end{center}

  Let's take another example and compare them. For 

  \[
	  B_m = \displaystyle \sum_{n}^{m} n^{- \dfrac{1}{2}}  
  \]

  We can prove that $B_m$ diverges too. Now let's compare $H_m$ and $B_m$ who diverges faster

  \[
	  \dfrac{dB_m}{dH_m} = \displaystyle \lim_{m \to \infty} \sqrt{m} = \infty
  \]

  So, obviously $B_m$ diverges faster(infinitely faster!) than $H_m$. Actually, both $H_m$ and $B_m$ is the member of 'Riemann Zeta Function' which is extremely crucial in number theory, and is the main topic of 'Riemann Hypothesis', but I am not gonna discuss Zeta Function too deep, let us just still fucos on the rate of divergence. For Zeta Function:

  \[
	  \zeta(s) = \displaystyle \sum_{n=1}^{\infty} \dfrac{1}{n^s} = \dfrac{1}{1^s} + \dfrac{1}{2^s} + \dfrac{1}{3^s} + ...
  \]

  Specifically, if $Re(s) > 1$ then $\zeta(s)$ converges. On the other hand, $Re(s) \leq 1$ then $\zeta(s)$ diverges. 

  When $Re(s) = 1$, zeta function would become $\zeta(1)$, namely, Harmonic Series. \\

  We can easily see that Harmonic Series has the slowest divergent rate in Zeta Function. As $s$ decreasing, divergent rate grows. 

  % here's a reference: https://www.zhihu.com/question/33700283






  \subsection{The rate of convergence}



\end{document}
